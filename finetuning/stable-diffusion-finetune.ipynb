{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16c61f54",
   "metadata": {},
   "source": [
    "# Finetune a stable diffusion model for text to image generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc23bae",
   "metadata": {},
   "source": [
    "***\n",
    "Welcome to Amazon [SageMaker JumpStart](https://docs.aws.amazon.com/sagemaker/latest/dg/studio-jumpstart.html)! You can use JumpStart to solve many Machine Learning tasks through one-click in SageMaker Studio, or through [SageMaker JumpStart API](https://sagemaker.readthedocs.io/en/stable/overview.html#use-prebuilt-models-with-sagemaker-jumpstart).  In this demo notebook, we demonstrate how to use the JumpStart API to generate images from text using state-of-the-art Stable Diffusion models. Furthermore, we show how to fine-tune the model to your dataset.\n",
    "\n",
    "Stable Diffusion is a text-to-image model that enables you to create photorealistic images from just a text prompt. A diffusion model trains by learning to remove noise that was added to a real image. This de-noising process generates a realistic image. These models can also generate images from text alone by conditioning the generation process on the text. For instance, Stable Diffusion is a latent diffusion where the model learns to recognize shapes in a pure noise image and gradually brings these shapes into focus if the shapes match the words in the input text.\n",
    "\n",
    "Training and deploying large models and running inference on models such as Stable Diffusion is often challenging and include issues such as cuda out of memory, payload size limit exceeded and so on.  JumpStart simplifies this process by providing ready-to-use scripts that have been robustly tested. Furthermore, it provides guidance on each step of the process including the recommended instance types, how to select parameters to guide image generation process, prompt engineering etc. Moreover, you can deploy and run inference on any of the 80+ Diffusion models from JumpStart without having to write any piece of your own code.\n",
    "\n",
    "\n",
    "In his notebook, you will learn how to use JumpStart to fine-tune the Stable Diffusion model to your dataset. This can be useful when creating art, logos, custom designs, NFTs, and so on, or fun stuff such as generating custom AI images of your pets or avatars of yourself.\n",
    "\n",
    "\n",
    "Model lincese: By using this model, you agree to the [CreativeML Open RAIL-M++ license](https://huggingface.co/stabilityai/stable-diffusion-2/blob/main/LICENSE-MODEL).\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db28351",
   "metadata": {},
   "source": [
    "1. [Set Up](#1.-Set-Up)\n",
    "2. [Fine-tune the pre-trained model on a custom dataset](#2.-Fine-tune-the-pre-trained-model-on-a-custom-dataset)\n",
    "    * [Retrieve Training Artifacts](#2.1.-Retrieve-Training-Artifacts)\n",
    "    * [Set Training parameters](#2.2.-Set-Training-parameters)\n",
    "    * [Start Training](#2.3.-Start-Training)\n",
    "    * [Deploy and run inference on the fine-tuned model](#3.2.-Deploy-and-run-inference-on-the-fine-tuned-model)\n",
    "3. [Conclusion](#3.-Conclusion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce462973",
   "metadata": {},
   "source": [
    "Note: This notebook was tested on ml.t3.medium instance in Amazon SageMaker Studio with Python 3 (Data Science) kernel and in Amazon SageMaker Notebook instance with conda_python3 kernel.\n",
    "\n",
    "Note: For fine-tuning the model on your dataset, you need `ml.g4dn.2xlarge` instance type available in your account. To deploy the fine-trained model, you can use `ml.p3.2xlarge` or `ml.g4dn.2xlarge` instance types. If `ml.g5.2xlarge` is available in your region, we recommend using that instance type for deployment. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea47727",
   "metadata": {},
   "source": [
    "## 1. Set Up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b91e81",
   "metadata": {},
   "source": [
    "***\n",
    "Before executing the notebook, there are some initial steps required for set up. This notebook requires latest version of sagemaker.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48370155",
   "metadata": {},
   "source": [
    "#### Permissions and environment variables\n",
    "\n",
    "***\n",
    "To host on Amazon SageMaker, we need to set up and authenticate the use of AWS services. Here, we use the execution role associated with the current notebook as the AWS account role with SageMaker access.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90518e45",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.25.0 is required for this version of SciPy (detected version 1.25.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import sagemaker, boto3, json\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "aws_role = get_execution_role()\n",
    "aws_region = boto3.Session().region_name\n",
    "sess = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8edfc4",
   "metadata": {},
   "source": [
    "## 2. Fine-tune the pre-trained model on a custom dataset\n",
    "\n",
    "---\n",
    "Previously, we saw how to run inference on a pre-trained model. Next, we discuss how a model can be finetuned to a custom dataset with any number of classes.\n",
    "\n",
    "The model can be fine-tuned to any dataset of images. It works very well even with as little as five training images.\n",
    "\n",
    "The fine-tuning script is built on the script from [dreambooth](https://dreambooth.github.io/). The model returned by fine-tuning can be further deployed for inference. Below are the instructions for how the training data should be formatted.\n",
    "\n",
    "- **Input:** A directory containing the instance images, `dataset_info.json` and (optional) directory `class_data_dir`.\n",
    "  - Images may be of `.png` or `.jpg` or `.jpeg` format.\n",
    "  - `dataset_info.json` file must be of the format {'instance_prompt':<<instance_prompt>>,'class_prompt':<<class_prompt>>}.\n",
    "  - If with_prior_preservation = False, you may choose to ignore 'class_prompt'.\n",
    "  - `class_data_dir` directory must have class images. If with_prior_preservation = True and class_data_dir is not present or there are not enough images already present in class_data_dir, additional images will be sampled with class_prompt.\n",
    "- **Output:** A trained model that can be deployed for inference.\n",
    "\n",
    "The s3 path should look like `s3://bucket_name/input_directory/`. Note the trailing `/` is required.\n",
    "\n",
    "Here is an example format of the training data.\n",
    "\n",
    "    input_directory\n",
    "        |---instance_image_1.png\n",
    "        |---instance_image_2.png\n",
    "        |---instance_image_3.png\n",
    "        |---instance_image_4.png\n",
    "        |---instance_image_5.png\n",
    "        |---dataset_info.json\n",
    "        |---class_data_dir\n",
    "            |---class_image_1.png\n",
    "            |---class_image_2.png\n",
    "            |---class_image_3.png\n",
    "            |---class_image_4.png\n",
    "\n",
    "**Prior preservation, instance prompt and class prompt:** Prior preservation is a technique that uses additional images of the same class that we are trying to train on.  For instance, if the training data consists of images of a particular dog, with prior preservation, we incorporate class images of generic dogs. It tries to avoid overfitting by showing images of different dogs while training for a particular dog. Tag indicating the specific dog present in instance prompt is missing in the class prompt. For instance, instance prompt may be \"a photo of a Doppler dog\" and class prompt may be \"a photo of a dog\". You can enable prior preservation by setting the hyper-parameter with_prior_preservation = True.\n",
    "\n",
    "\n",
    "\n",
    "We provide a default dataset of dog images. It consists of images (instance images corresponding to instance prompt) of a single dog with no class images. If using the default dataset, try the prompt \"a photo of a Doppler dog\" while doing inference in the demo notebook.\n",
    "\n",
    "\n",
    "License: [MIT](https://github.com/marshmellow77/dreambooth-sm/blob/main/LICENSE)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bfaa4d",
   "metadata": {},
   "source": [
    "### 2.1. Retrieve Training Artifacts\n",
    "\n",
    "---\n",
    "Here, we retrieve the training docker container, the training algorithm source, and the pre-trained base model. Note that model_version=\"*\" fetches the latest model.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f11ff722",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker import image_uris, model_uris, script_uris\n",
    "\n",
    "# Currently, not all the stable diffusion models in jumpstart support finetuning. Thus, we manually select a model\n",
    "# which supports finetuning.\n",
    "train_model_id, train_model_version, train_scope = (\n",
    "    \"model-txt2img-stabilityai-stable-diffusion-v2-1-base\",\n",
    "    \"*\",\n",
    "    \"training\",\n",
    ")\n",
    "\n",
    "# Tested with ml.p3.2xlarge (),ml.g4dn.2xlarge (16GB GPU memory),and ml.g5.2xlarge (24GB GPU memory) instances. Other instances may work as well.\n",
    "# If ml.g5.2xlarge instance type is available, please change the following instance type to speed up training. Note: If using Workshop Studio accounts,\n",
    "# leave the default instance below.\n",
    "training_instance_type = \"ml.p3.2xlarge\"\n",
    "\n",
    "# Retrieve the docker image\n",
    "train_image_uri = image_uris.retrieve(\n",
    "    region=None,\n",
    "    framework=None,  # automatically inferred from model_id\n",
    "    model_id=train_model_id,\n",
    "    model_version=train_model_version,\n",
    "    image_scope=train_scope,\n",
    "    instance_type=training_instance_type,\n",
    ")\n",
    "\n",
    "# Retrieve the training script. This contains all the necessary files including data processing, model training etc.\n",
    "train_source_uri = script_uris.retrieve(\n",
    "    model_id=train_model_id, model_version=train_model_version, script_scope=train_scope\n",
    ")\n",
    "# Retrieve the pre-trained model tarball to further fine-tune\n",
    "train_model_uri = model_uris.retrieve(\n",
    "    model_id=train_model_id, model_version=train_model_version, model_scope=train_scope\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6dd7c7-f788-4cfd-9d66-f768a28098b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the location of the fine tuning scripts\n",
    "train_source_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fc5eae-94a4-441c-813a-a0f13521df1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the fine tuning scripts\n",
    "!aws s3 cp s3://jumpstart-cache-prod-us-east-1/source-directory-tarballs/stabilityai/transfer_learning/txt2img/prepack/v1.0.3/sourcedir.tar.gz ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab6334f-500c-4c43-a6f3-8b12491e9d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# untar the file to see the information\n",
    "!tar -xvf sourcedir.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e266289",
   "metadata": {},
   "source": [
    "### 2.2. Set Training parameters\n",
    "\n",
    "---\n",
    "Now that we are done with all the set up that is needed, we are ready to train our stable diffusion model. To begin, let us create a [``sageMaker.estimator.Estimator``](https://sagemaker.readthedocs.io/en/stable/api/training/estimators.html) object. This estimator will launch the training job.\n",
    "\n",
    "There are two kinds of parameters that need to be set for training. The first one are the parameters for the training job. These include: (i) Training data path. This is S3 folder in which the input data is stored, (ii) Output path: This the s3 folder in which the training output is stored. (iii) Training instance type: This indicates the type of machine on which to run the training. We defined the training instance type above to fetch the correct train_image_uri.\n",
    "\n",
    "The second set of parameters are algorithm specific training hyper-parameters.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e21c709f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sample training data is available in this bucket\n",
    "training_data_bucket = f\"jumpstart-cache-prod-{aws_region}\"\n",
    "training_data_prefix = \"training-datasets/dogs_sd_finetuning/\"\n",
    "\n",
    "training_dataset_s3_path = f\"s3://{training_data_bucket}/{training_data_prefix}\"\n",
    "\n",
    "output_bucket = sess.default_bucket()\n",
    "output_prefix = \"jumpstart-example-sd-training\"\n",
    "\n",
    "s3_output_location = f\"s3://{output_bucket}/{output_prefix}/output\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adda2a1e",
   "metadata": {},
   "source": [
    "---\n",
    "For algorithm specific hyper-parameters, we start by fetching python dictionary of the training hyper-parameters that the algorithm accepts with their default values. This can then be overridden to custom values.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa371787",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epochs': '20', 'max_steps': '400', 'batch_size': '1', 'with_prior_preservation': 'False', 'num_class_images': '100', 'learning_rate': '2e-06', 'prior_loss_weight': '1.0', 'center_crop': 'False', 'lr_scheduler': 'constant', 'adam_weight_decay': '0.01', 'adam_beta1': '0.9', 'adam_beta2': '0.999', 'adam_epsilon': '1e-08', 'gradient_accumulation_steps': '1', 'max_grad_norm': '1.0', 'compute_fid': 'False', 'seed': '0'}\n"
     ]
    }
   ],
   "source": [
    "from sagemaker import hyperparameters\n",
    "\n",
    "# Retrieve the default hyper-parameters for fine-tuning the model\n",
    "hyperparameters = hyperparameters.retrieve_default(\n",
    "    model_id=train_model_id, model_version=train_model_version\n",
    ")\n",
    "\n",
    "# [Optional] Override default hyperparameters with custom values\n",
    "hyperparameters[\"max_steps\"] = \"400\"\n",
    "print(hyperparameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d102c884",
   "metadata": {},
   "source": [
    "---\n",
    "If setting `with_prior_preservation=True`, please use ml.g5.2xlarge instance type as more memory is required to generate class images. Currently, training on ml.g4dn.2xlarge instance type run into CUDA out of memory issue when setting `with_prior_preservation=True`.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cda2854",
   "metadata": {},
   "source": [
    "### 2.3. Start Training\n",
    "---\n",
    "We start by creating the estimator object with all the required assets and then launch the training job.  It takes less than 10 mins on the default dataset.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76bdbb83",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: jumpstart-example-model-txt2img-stabili-2023-09-08-17-44-58-039\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-09-08 17:44:58 Starting - Starting the training job...\n",
      "2023-09-08 17:45:25 Starting - Preparing the instances for training.........\n",
      "2023-09-08 17:46:35 Downloading - Downloading input data..................\n",
      "2023-09-08 17:49:36 Training - Downloading the training image.........\n",
      "2023-09-08 17:51:11 Training - Training image download completed. Training in progress...\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2023-09-08 17:51:36,301 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2023-09-08 17:51:36,331 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2023-09-08 17:51:36,333 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2023-09-08 17:51:38,224 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.8 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mProcessing ./lib/accelerate/accelerate-0.15.0-py3-none-any.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/bitsandbytes/bitsandbytes-0.38.1-py3-none-any.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/datasets/datasets-2.8.0-py3-none-any.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/diffusers/diffusers-0.11.1-py3-none-any.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pytorch-fid/pytorch_fid-0.3.0-py3-none-any.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/transformers/transformers-4.25.1-py3-none-any.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/sagemaker_jumpstart_script_utilities/sagemaker_jumpstart_script_utilities-1.1.4-py2.py3-none-any.whl\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch>=1.4.0 in /opt/conda/lib/python3.8/site-packages (from accelerate==0.15.0->-r requirements.txt (line 1)) (1.10.2+cu113)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from accelerate==0.15.0->-r requirements.txt (line 1)) (21.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.8/site-packages (from accelerate==0.15.0->-r requirements.txt (line 1)) (5.9.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.8/site-packages (from accelerate==0.15.0->-r requirements.txt (line 1)) (1.22.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml in /opt/conda/lib/python3.8/site-packages (from accelerate==0.15.0->-r requirements.txt (line 1)) (5.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=6.0.0 in /opt/conda/lib/python3.8/site-packages (from datasets==2.8.0->-r requirements.txt (line 3)) (9.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.8/site-packages (from datasets==2.8.0->-r requirements.txt (line 3)) (4.64.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /opt/conda/lib/python3.8/site-packages (from datasets==2.8.0->-r requirements.txt (line 3)) (0.10.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: xxhash in /opt/conda/lib/python3.8/site-packages (from datasets==2.8.0->-r requirements.txt (line 3)) (3.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.8/site-packages (from datasets==2.8.0->-r requirements.txt (line 3)) (0.70.13)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.8/site-packages (from datasets==2.8.0->-r requirements.txt (line 3)) (1.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiohttp in /opt/conda/lib/python3.8/site-packages (from datasets==2.8.0->-r requirements.txt (line 3)) (3.8.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.8/site-packages (from datasets==2.8.0->-r requirements.txt (line 3)) (0.18.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill<0.3.7 in /opt/conda/lib/python3.8/site-packages (from datasets==2.8.0->-r requirements.txt (line 3)) (0.3.5.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.8/site-packages (from datasets==2.8.0->-r requirements.txt (line 3)) (2.28.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/lib/python3.8/site-packages (from datasets==2.8.0->-r requirements.txt (line 3)) (2022.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.8/site-packages (from diffusers==0.11.1->-r requirements.txt (line 4)) (3.8.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.8/site-packages (from diffusers==0.11.1->-r requirements.txt (line 4)) (2022.9.13)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.8/site-packages (from diffusers==0.11.1->-r requirements.txt (line 4)) (4.12.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: Pillow in /opt/conda/lib/python3.8/site-packages (from diffusers==0.11.1->-r requirements.txt (line 4)) (9.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scipy in /opt/conda/lib/python3.8/site-packages (from pytorch-fid==0.3.0->-r requirements.txt (line 5)) (1.9.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torchvision>=0.2.2 in /opt/conda/lib/python3.8/site-packages (from pytorch-fid==0.3.0->-r requirements.txt (line 5)) (0.11.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.8/site-packages (from transformers==4.25.1->-r requirements.txt (line 6)) (0.13.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets==2.8.0->-r requirements.txt (line 3)) (1.8.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<3.0,>=2.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets==2.8.0->-r requirements.txt (line 3)) (2.0.12)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets==2.8.0->-r requirements.txt (line 3)) (21.4.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets==2.8.0->-r requirements.txt (line 3)) (1.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets==2.8.0->-r requirements.txt (line 3)) (4.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets==2.8.0->-r requirements.txt (line 3)) (1.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets==2.8.0->-r requirements.txt (line 3)) (6.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets==2.8.0->-r requirements.txt (line 3)) (4.3.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging>=20.0->accelerate==0.15.0->-r requirements.txt (line 1)) (3.0.9)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->datasets==2.8.0->-r requirements.txt (line 3)) (2022.9.24)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->datasets==2.8.0->-r requirements.txt (line 3)) (3.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->datasets==2.8.0->-r requirements.txt (line 3)) (1.26.12)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.8/site-packages (from importlib-metadata->diffusers==0.11.1->-r requirements.txt (line 4)) (3.8.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets==2.8.0->-r requirements.txt (line 3)) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets==2.8.0->-r requirements.txt (line 3)) (2022.2.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.8.1->pandas->datasets==2.8.0->-r requirements.txt (line 3)) (1.16.0)\u001b[0m\n",
      "\u001b[34mInstalling collected packages: bitsandbytes, sagemaker-jumpstart-script-utilities, accelerate, transformers, pytorch-fid, diffusers, datasets\u001b[0m\n",
      "\u001b[34mAttempting uninstall: transformers\u001b[0m\n",
      "\u001b[34mFound existing installation: transformers 4.17.0\u001b[0m\n",
      "\u001b[34mUninstalling transformers-4.17.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled transformers-4.17.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: datasets\u001b[0m\n",
      "\u001b[34mFound existing installation: datasets 1.18.4\u001b[0m\n",
      "\u001b[34mUninstalling datasets-1.18.4:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled datasets-1.18.4\u001b[0m\n",
      "\u001b[34mSuccessfully installed accelerate-0.15.0 bitsandbytes-0.38.1 datasets-2.8.0 diffusers-0.11.1 pytorch-fid-0.3.0 sagemaker-jumpstart-script-utilities-1.1.4 transformers-4.25.1\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip available: 22.2.2 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m2023-09-08 17:51:49,754 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-09-08 17:51:49,754 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-09-08 17:51:49,880 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"model\": \"/opt/ml/input/data/model\",\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.p3.2xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"adam_beta1\": \"0.9\",\n",
      "        \"adam_beta2\": \"0.999\",\n",
      "        \"adam_epsilon\": \"1e-08\",\n",
      "        \"adam_weight_decay\": \"0.01\",\n",
      "        \"batch_size\": \"1\",\n",
      "        \"center_crop\": \"False\",\n",
      "        \"compute_fid\": \"False\",\n",
      "        \"epochs\": \"20\",\n",
      "        \"gradient_accumulation_steps\": \"1\",\n",
      "        \"learning_rate\": \"2e-06\",\n",
      "        \"lr_scheduler\": \"constant\",\n",
      "        \"max_grad_norm\": \"1.0\",\n",
      "        \"max_steps\": \"400\",\n",
      "        \"num_class_images\": \"100\",\n",
      "        \"prior_loss_weight\": \"1.0\",\n",
      "        \"seed\": \"0\",\n",
      "        \"with_prior_preservation\": \"False\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"model\": {\n",
      "            \"ContentType\": \"application/x-sagemaker-model\",\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.p3.2xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"job_name\": \"jumpstart-example-model-txt2img-stabili-2023-09-08-17-44-58-039\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://jumpstart-cache-prod-us-east-1/source-directory-tarballs/stabilityai/transfer_learning/txt2img/prepack/v1.0.3/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"transfer_learning\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.p3.2xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p3.2xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"transfer_learning.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"adam_beta1\":\"0.9\",\"adam_beta2\":\"0.999\",\"adam_epsilon\":\"1e-08\",\"adam_weight_decay\":\"0.01\",\"batch_size\":\"1\",\"center_crop\":\"False\",\"compute_fid\":\"False\",\"epochs\":\"20\",\"gradient_accumulation_steps\":\"1\",\"learning_rate\":\"2e-06\",\"lr_scheduler\":\"constant\",\"max_grad_norm\":\"1.0\",\"max_steps\":\"400\",\"num_class_images\":\"100\",\"prior_loss_weight\":\"1.0\",\"seed\":\"0\",\"with_prior_preservation\":\"False\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=transfer_learning.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"model\":{\"ContentType\":\"application/x-sagemaker-model\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"model\",\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.p3.2xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=transfer_learning\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://jumpstart-cache-prod-us-east-1/source-directory-tarballs/stabilityai/transfer_learning/txt2img/prepack/v1.0.3/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"model\":\"/opt/ml/input/data/model\",\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.p3.2xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"adam_beta1\":\"0.9\",\"adam_beta2\":\"0.999\",\"adam_epsilon\":\"1e-08\",\"adam_weight_decay\":\"0.01\",\"batch_size\":\"1\",\"center_crop\":\"False\",\"compute_fid\":\"False\",\"epochs\":\"20\",\"gradient_accumulation_steps\":\"1\",\"learning_rate\":\"2e-06\",\"lr_scheduler\":\"constant\",\"max_grad_norm\":\"1.0\",\"max_steps\":\"400\",\"num_class_images\":\"100\",\"prior_loss_weight\":\"1.0\",\"seed\":\"0\",\"with_prior_preservation\":\"False\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"model\":{\"ContentType\":\"application/x-sagemaker-model\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"job_name\":\"jumpstart-example-model-txt2img-stabili-2023-09-08-17-44-58-039\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://jumpstart-cache-prod-us-east-1/source-directory-tarballs/stabilityai/transfer_learning/txt2img/prepack/v1.0.3/sourcedir.tar.gz\",\"module_name\":\"transfer_learning\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"transfer_learning.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--adam_beta1\",\"0.9\",\"--adam_beta2\",\"0.999\",\"--adam_epsilon\",\"1e-08\",\"--adam_weight_decay\",\"0.01\",\"--batch_size\",\"1\",\"--center_crop\",\"False\",\"--compute_fid\",\"False\",\"--epochs\",\"20\",\"--gradient_accumulation_steps\",\"1\",\"--learning_rate\",\"2e-06\",\"--lr_scheduler\",\"constant\",\"--max_grad_norm\",\"1.0\",\"--max_steps\",\"400\",\"--num_class_images\",\"100\",\"--prior_loss_weight\",\"1.0\",\"--seed\",\"0\",\"--with_prior_preservation\",\"False\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_MODEL=/opt/ml/input/data/model\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_ADAM_BETA1=0.9\u001b[0m\n",
      "\u001b[34mSM_HP_ADAM_BETA2=0.999\u001b[0m\n",
      "\u001b[34mSM_HP_ADAM_EPSILON=1e-08\u001b[0m\n",
      "\u001b[34mSM_HP_ADAM_WEIGHT_DECAY=0.01\u001b[0m\n",
      "\u001b[34mSM_HP_BATCH_SIZE=1\u001b[0m\n",
      "\u001b[34mSM_HP_CENTER_CROP=False\u001b[0m\n",
      "\u001b[34mSM_HP_COMPUTE_FID=False\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=20\u001b[0m\n",
      "\u001b[34mSM_HP_GRADIENT_ACCUMULATION_STEPS=1\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING_RATE=2e-06\u001b[0m\n",
      "\u001b[34mSM_HP_LR_SCHEDULER=constant\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_GRAD_NORM=1.0\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_STEPS=400\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_CLASS_IMAGES=100\u001b[0m\n",
      "\u001b[34mSM_HP_PRIOR_LOSS_WEIGHT=1.0\u001b[0m\n",
      "\u001b[34mSM_HP_SEED=0\u001b[0m\n",
      "\u001b[34mSM_HP_WITH_PRIOR_PRESERVATION=False\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.22b20220929-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument-3.4.2-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument_cext-0.2.4-py3.8-linux-x86_64.egg\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.8 transfer_learning.py --adam_beta1 0.9 --adam_beta2 0.999 --adam_epsilon 1e-08 --adam_weight_decay 0.01 --batch_size 1 --center_crop False --compute_fid False --epochs 20 --gradient_accumulation_steps 1 --learning_rate 2e-06 --lr_scheduler constant --max_grad_norm 1.0 --max_steps 400 --num_class_images 100 --prior_loss_weight 1.0 --seed 0 --with_prior_preservation False\u001b[0m\n",
      "\u001b[34m[2023-09-08 17:51:51.738: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:68] Found unsupported HuggingFace version 4.25.1 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34mRunning training scripts with arguments: Namespace(adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, adam_weight_decay=0.01, batch_size=1, center_crop=False, class_data_dir='class_data_dir', compute_fid=False, epochs=20, gradient_accumulation_steps=1, gradient_checkpointing=True, learning_rate=2e-06, local_rank=-1, lr_scheduler='constant', lr_warmup_steps=500, max_grad_norm=1.0, max_steps=400, mixed_precision=None, model_dir='/opt/ml/model', num_class_images=100, pretrained_model='/opt/ml/input/data/model', prior_generation_precision=None, prior_loss_weight=1.0, scale_lr=False, seed=0, train='/opt/ml/input/data/training', train_alt=None, train_text_encoder=False, use_8bit_adam=True, with_prior_preservation=False).\u001b[0m\n",
      "\u001b[34mIgnoring unrecognized arguments: [].\u001b[0m\n",
      "\u001b[34mRunning without prior preservation. It may cause the model to overfit or experience language drift.\u001b[0m\n",
      "\u001b[34mLoading model and optimizer.\u001b[0m\n",
      "\u001b[34mYou are using a model of type clip_text_model to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\u001b[0m\n",
      "\u001b[34m{'variance_type'} was not found in config. Values will be initialized to default values.\u001b[0m\n",
      "\u001b[34mEnabling gradient checkpointing for unet.\u001b[0m\n",
      "\u001b[34m===================================BUG REPORT===================================\u001b[0m\n",
      "\u001b[34mWelcome to bitsandbytes. For bug reports, please run\u001b[0m\n",
      "\u001b[34mpython -m bitsandbytes\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\u001b[0m\n",
      "\u001b[34m================================================================================\u001b[0m\n",
      "\u001b[34mbin /opt/conda/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda113_nocublaslt.so\u001b[0m\n",
      "\u001b[34mCUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\u001b[0m\n",
      "\u001b[34mCUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so.11.0\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Highest compute capability among GPUs detected: 7.0\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Detected CUDA version 113\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Loading binary /opt/conda/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda113_nocublaslt.so...\u001b[0m\n",
      "\u001b[34mUsing AdamW8bit optimizer for training.\u001b[0m\n",
      "\u001b[34mPreparing unet, optimizer, train dataloader and lr scheduler with accelerator.\u001b[0m\n",
      "\u001b[34mMoving text encoder to cuda device.\u001b[0m\n",
      "\u001b[34m0%|          | 0/400 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mSteps:   0%|          | 0/400 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mEpoch: 0\u001b[0m\n",
      "\u001b[34m[2023-09-08 17:52:55.338: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:68] Found unsupported HuggingFace version 4.25.1 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[2023-09-08 17:52:55.393 algo-1:58 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-09-08 17:52:55.553 algo-1:58 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2023-09-08 17:52:55.555 algo-1:58 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2023-09-08 17:52:55.555 algo-1:58 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2023-09-08 17:52:55.556 algo-1:58 INFO hook.py:254] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2023-09-08 17:52:55.556 algo-1:58 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34mSteps:   0%|          | 1/400 [00:02<15:40,  2.36s/it]\u001b[0m\n",
      "\u001b[34mSteps:   0%|          | 1/400 [00:02<15:40,  2.36s/it, loss=0.0474, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:   0%|          | 2/400 [00:02<08:16,  1.25s/it, loss=0.0474, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:   0%|          | 2/400 [00:02<08:16,  1.25s/it, loss=0.169, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:   1%|          | 3/400 [00:03<05:59,  1.10it/s, loss=0.169, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:   1%|          | 3/400 [00:03<05:59,  1.10it/s, loss=0.406, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:   1%|          | 4/400 [00:03<04:52,  1.36it/s, loss=0.406, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:   1%|          | 4/400 [00:03<04:52,  1.36it/s, loss=0.00587, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:   1%|▏         | 5/400 [00:04<04:15,  1.54it/s, loss=0.00587, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:   1%|▏         | 5/400 [00:04<04:15,  1.54it/s, loss=0.147, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:   2%|▏         | 6/400 [00:04<03:53,  1.68it/s, loss=0.147, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:   2%|▏         | 6/400 [00:04<03:53,  1.68it/s, loss=0.00716, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:   2%|▏         | 7/400 [00:05<03:40,  1.79it/s, loss=0.00716, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:   2%|▏         | 7/400 [00:05<03:40,  1.79it/s, loss=0.0928, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:   2%|▏         | 8/400 [00:05<03:31,  1.85it/s, loss=0.0928, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:   2%|▏         | 8/400 [00:05<03:31,  1.85it/s, loss=0.279, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:   2%|▏         | 9/400 [00:06<03:25,  1.90it/s, loss=0.279, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:   2%|▏         | 9/400 [00:06<03:25,  1.90it/s, loss=0.00397, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:   2%|▎         | 10/400 [00:06<03:19,  1.95it/s, loss=0.00397, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:   2%|▎         | 10/400 [00:06<03:19,  1.95it/s, loss=0.111, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:   3%|▎         | 11/400 [00:07<03:16,  1.98it/s, loss=0.111, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:   3%|▎         | 11/400 [00:07<03:16,  1.98it/s, loss=0.45, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:   3%|▎         | 12/400 [00:07<03:15,  1.99it/s, loss=0.45, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:   3%|▎         | 12/400 [00:07<03:15,  1.99it/s, loss=0.279, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:   3%|▎         | 13/400 [00:08<03:13,  2.00it/s, loss=0.279, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:   3%|▎         | 13/400 [00:08<03:13,  2.00it/s, loss=0.0367, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:   4%|▎         | 14/400 [00:08<03:12,  2.01it/s, loss=0.0367, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:   4%|▎         | 14/400 [00:08<03:12,  2.01it/s, loss=0.497, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:   4%|▍         | 15/400 [00:09<03:11,  2.01it/s, loss=0.497, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:   4%|▍         | 15/400 [00:09<03:11,  2.01it/s, loss=0.019, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:   4%|▍         | 16/400 [00:09<03:09,  2.02it/s, loss=0.019, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:   4%|▍         | 16/400 [00:09<03:09,  2.02it/s, loss=0.13, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:   4%|▍         | 17/400 [00:10<03:07,  2.04it/s, loss=0.13, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:   4%|▍         | 17/400 [00:10<03:07,  2.04it/s, loss=0.00323, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:   4%|▍         | 18/400 [00:10<03:06,  2.05it/s, loss=0.00323, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:   4%|▍         | 18/400 [00:10<03:06,  2.05it/s, loss=0.156, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:   5%|▍         | 19/400 [00:11<03:14,  1.96it/s, loss=0.156, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:   5%|▍         | 19/400 [00:11<03:14,  1.96it/s, loss=0.26, lr=2e-6]\u001b[0m\n",
      "\u001b[34mEnd epoch 0: train_avg_loss=0.1631894343483605\u001b[0m\n",
      "\u001b[34mEpoch: 1\u001b[0m\n",
      "\u001b[34mSteps:   5%|▌         | 20/400 [00:12<04:03,  1.56it/s, loss=0.26, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:   5%|▌         | 20/400 [00:12<04:03,  1.56it/s, loss=0.0479, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:   5%|▌         | 21/400 [00:12<03:45,  1.68it/s, loss=0.0479, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:   5%|▌         | 21/400 [00:12<03:45,  1.68it/s, loss=0.292, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:   6%|▌         | 22/400 [00:13<03:32,  1.78it/s, loss=0.292, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:   6%|▌         | 22/400 [00:13<03:32,  1.78it/s, loss=0.0404, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:   6%|▌         | 23/400 [00:13<03:23,  1.85it/s, loss=0.0404, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:   6%|▌         | 23/400 [00:13<03:23,  1.85it/s, loss=0.0194, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:   6%|▌         | 24/400 [00:14<03:16,  1.91it/s, loss=0.0194, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:   6%|▌         | 24/400 [00:14<03:16,  1.91it/s, loss=0.26, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:   6%|▋         | 25/400 [00:14<03:12,  1.95it/s, loss=0.26, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:   6%|▋         | 25/400 [00:14<03:12,  1.95it/s, loss=0.373, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:   6%|▋         | 26/400 [00:15<03:08,  1.98it/s, loss=0.373, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:   6%|▋         | 26/400 [00:15<03:08,  1.98it/s, loss=0.323, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:   7%|▋         | 27/400 [00:15<03:06,  2.00it/s, loss=0.323, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:   7%|▋         | 27/400 [00:15<03:06,  2.00it/s, loss=0.0724, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:   7%|▋         | 28/400 [00:16<03:04,  2.02it/s, loss=0.0724, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:   7%|▋         | 28/400 [00:16<03:04,  2.02it/s, loss=0.174, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:   7%|▋         | 29/400 [00:16<03:02,  2.03it/s, loss=0.174, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:   7%|▋         | 29/400 [00:16<03:02,  2.03it/s, loss=0.144, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:   8%|▊         | 30/400 [00:17<03:01,  2.04it/s, loss=0.144, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:   8%|▊         | 30/400 [00:17<03:01,  2.04it/s, loss=0.28, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:   8%|▊         | 31/400 [00:17<03:00,  2.04it/s, loss=0.28, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:   8%|▊         | 31/400 [00:17<03:00,  2.04it/s, loss=0.0244, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:   8%|▊         | 32/400 [00:18<03:01,  2.03it/s, loss=0.0244, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:   8%|▊         | 32/400 [00:18<03:01,  2.03it/s, loss=0.0651, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:   8%|▊         | 33/400 [00:18<02:59,  2.05it/s, loss=0.0651, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:   8%|▊         | 33/400 [00:18<02:59,  2.05it/s, loss=0.00298, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:   8%|▊         | 34/400 [00:18<02:59,  2.04it/s, loss=0.00298, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:   8%|▊         | 34/400 [00:19<02:59,  2.04it/s, loss=0.293, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:   9%|▉         | 35/400 [00:19<02:58,  2.04it/s, loss=0.293, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:   9%|▉         | 35/400 [00:19<02:58,  2.04it/s, loss=0.151, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:   9%|▉         | 36/400 [00:19<02:59,  2.03it/s, loss=0.151, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:   9%|▉         | 36/400 [00:20<02:59,  2.03it/s, loss=0.0409, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:   9%|▉         | 37/400 [00:20<02:57,  2.04it/s, loss=0.0409, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:   9%|▉         | 37/400 [00:20<02:57,  2.04it/s, loss=0.0228, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  10%|▉         | 38/400 [00:21<03:03,  1.97it/s, loss=0.0228, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  10%|▉         | 38/400 [00:21<03:03,  1.97it/s, loss=0.266, lr=2e-6]\u001b[0m\n",
      "\u001b[34mEnd epoch 1: train_avg_loss=0.15223886499083356\u001b[0m\n",
      "\u001b[34mEpoch: 2\u001b[0m\n",
      "\u001b[34mSteps:  10%|▉         | 39/400 [00:21<03:52,  1.55it/s, loss=0.266, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  10%|▉         | 39/400 [00:22<03:52,  1.55it/s, loss=0.00916, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  10%|█         | 40/400 [00:22<03:35,  1.67it/s, loss=0.00916, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  10%|█         | 40/400 [00:22<03:35,  1.67it/s, loss=0.0735, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  10%|█         | 41/400 [00:22<03:23,  1.76it/s, loss=0.0735, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  10%|█         | 41/400 [00:22<03:23,  1.76it/s, loss=0.112, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  10%|█         | 42/400 [00:23<03:15,  1.83it/s, loss=0.112, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  10%|█         | 42/400 [00:23<03:15,  1.83it/s, loss=0.372, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  11%|█         | 43/400 [00:23<03:08,  1.89it/s, loss=0.372, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  11%|█         | 43/400 [00:23<03:08,  1.89it/s, loss=0.00488, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  11%|█         | 44/400 [00:24<03:04,  1.93it/s, loss=0.00488, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  11%|█         | 44/400 [00:24<03:04,  1.93it/s, loss=0.0776, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  11%|█▏        | 45/400 [00:24<02:59,  1.98it/s, loss=0.0776, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  11%|█▏        | 45/400 [00:24<02:59,  1.98it/s, loss=0.0112, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  12%|█▏        | 46/400 [00:25<02:57,  2.00it/s, loss=0.0112, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  12%|█▏        | 46/400 [00:25<02:57,  2.00it/s, loss=0.0137, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  12%|█▏        | 47/400 [00:25<02:55,  2.01it/s, loss=0.0137, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  12%|█▏        | 47/400 [00:25<02:55,  2.01it/s, loss=0.331, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  12%|█▏        | 48/400 [00:26<02:55,  2.01it/s, loss=0.331, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  12%|█▏        | 48/400 [00:26<02:55,  2.01it/s, loss=0.0893, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  12%|█▏        | 49/400 [00:26<02:53,  2.02it/s, loss=0.0893, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  12%|█▏        | 49/400 [00:26<02:53,  2.02it/s, loss=0.0453, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  12%|█▎        | 50/400 [00:27<02:51,  2.04it/s, loss=0.0453, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  12%|█▎        | 50/400 [00:27<02:51,  2.04it/s, loss=0.22, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  13%|█▎        | 51/400 [00:27<02:51,  2.04it/s, loss=0.22, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  13%|█▎        | 51/400 [00:27<02:51,  2.04it/s, loss=0.0866, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  13%|█▎        | 52/400 [00:28<02:50,  2.04it/s, loss=0.0866, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  13%|█▎        | 52/400 [00:28<02:50,  2.04it/s, loss=0.0552, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  13%|█▎        | 53/400 [00:28<02:49,  2.04it/s, loss=0.0552, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  13%|█▎        | 53/400 [00:28<02:49,  2.04it/s, loss=0.304, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  14%|█▎        | 54/400 [00:29<02:50,  2.03it/s, loss=0.304, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  14%|█▎        | 54/400 [00:29<02:50,  2.03it/s, loss=0.232, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  14%|█▍        | 55/400 [00:29<02:49,  2.03it/s, loss=0.232, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  14%|█▍        | 55/400 [00:29<02:49,  2.03it/s, loss=0.0947, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  14%|█▍        | 56/400 [00:30<02:49,  2.03it/s, loss=0.0947, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  14%|█▍        | 56/400 [00:30<02:49,  2.03it/s, loss=0.139, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  14%|█▍        | 57/400 [00:30<02:55,  1.95it/s, loss=0.139, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  14%|█▍        | 57/400 [00:30<02:55,  1.95it/s, loss=0.00266, lr=2e-6]\u001b[0m\n",
      "\u001b[34mEnd epoch 2: train_avg_loss=0.11961822954349612\u001b[0m\n",
      "\u001b[34mEpoch: 3\u001b[0m\n",
      "\u001b[34mSteps:  14%|█▍        | 58/400 [00:31<03:41,  1.55it/s, loss=0.00266, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  14%|█▍        | 58/400 [00:31<03:41,  1.55it/s, loss=0.0407, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  15%|█▍        | 59/400 [00:32<03:23,  1.67it/s, loss=0.0407, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  15%|█▍        | 59/400 [00:32<03:23,  1.67it/s, loss=0.149, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  15%|█▌        | 60/400 [00:32<03:12,  1.77it/s, loss=0.149, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  15%|█▌        | 60/400 [00:32<03:12,  1.77it/s, loss=0.0412, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  15%|█▌        | 61/400 [00:33<03:04,  1.84it/s, loss=0.0412, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  15%|█▌        | 61/400 [00:33<03:04,  1.84it/s, loss=0.00655, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  16%|█▌        | 62/400 [00:33<02:59,  1.89it/s, loss=0.00655, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  16%|█▌        | 62/400 [00:33<02:59,  1.89it/s, loss=0.612, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  16%|█▌        | 63/400 [00:34<02:54,  1.93it/s, loss=0.612, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  16%|█▌        | 63/400 [00:34<02:54,  1.93it/s, loss=0.247, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  16%|█▌        | 64/400 [00:34<02:51,  1.96it/s, loss=0.247, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  16%|█▌        | 64/400 [00:34<02:51,  1.96it/s, loss=0.012, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  16%|█▋        | 65/400 [00:35<02:48,  1.98it/s, loss=0.012, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  16%|█▋        | 65/400 [00:35<02:48,  1.98it/s, loss=0.0889, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  16%|█▋        | 66/400 [00:35<02:48,  1.98it/s, loss=0.0889, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  16%|█▋        | 66/400 [00:35<02:48,  1.98it/s, loss=0.0742, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  17%|█▋        | 67/400 [00:36<02:45,  2.01it/s, loss=0.0742, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  17%|█▋        | 67/400 [00:36<02:45,  2.01it/s, loss=0.0101, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  17%|█▋        | 68/400 [00:36<02:44,  2.02it/s, loss=0.0101, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  17%|█▋        | 68/400 [00:36<02:44,  2.02it/s, loss=0.0227, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  17%|█▋        | 69/400 [00:37<02:43,  2.03it/s, loss=0.0227, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  17%|█▋        | 69/400 [00:37<02:43,  2.03it/s, loss=0.116, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  18%|█▊        | 70/400 [00:37<02:42,  2.03it/s, loss=0.116, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  18%|█▊        | 70/400 [00:37<02:42,  2.03it/s, loss=0.00226, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  18%|█▊        | 71/400 [00:38<02:41,  2.03it/s, loss=0.00226, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  18%|█▊        | 71/400 [00:38<02:41,  2.03it/s, loss=0.0151, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  18%|█▊        | 72/400 [00:38<02:40,  2.04it/s, loss=0.0151, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  18%|█▊        | 72/400 [00:38<02:40,  2.04it/s, loss=0.0067, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  18%|█▊        | 73/400 [00:39<02:41,  2.03it/s, loss=0.0067, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  18%|█▊        | 73/400 [00:39<02:41,  2.03it/s, loss=0.229, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  18%|█▊        | 74/400 [00:39<02:39,  2.04it/s, loss=0.229, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  18%|█▊        | 74/400 [00:39<02:39,  2.04it/s, loss=0.00241, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  19%|█▉        | 75/400 [00:40<02:39,  2.04it/s, loss=0.00241, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  19%|█▉        | 75/400 [00:40<02:39,  2.04it/s, loss=0.0544, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  19%|█▉        | 76/400 [00:40<02:44,  1.97it/s, loss=0.0544, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  19%|█▉        | 76/400 [00:40<02:44,  1.97it/s, loss=0.0103, lr=2e-6]\u001b[0m\n",
      "\u001b[34mEnd epoch 3: train_avg_loss=0.09157079127372096\u001b[0m\n",
      "\u001b[34mEpoch: 4\u001b[0m\n",
      "\u001b[34mSteps:  19%|█▉        | 77/400 [00:41<03:23,  1.59it/s, loss=0.0103, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  19%|█▉        | 77/400 [00:41<03:23,  1.59it/s, loss=0.00686, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  20%|█▉        | 78/400 [00:42<03:09,  1.70it/s, loss=0.00686, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  20%|█▉        | 78/400 [00:42<03:09,  1.70it/s, loss=0.233, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  20%|█▉        | 79/400 [00:42<02:59,  1.79it/s, loss=0.233, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  20%|█▉        | 79/400 [00:42<02:59,  1.79it/s, loss=0.0147, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  20%|██        | 80/400 [00:43<02:52,  1.85it/s, loss=0.0147, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  20%|██        | 80/400 [00:43<02:52,  1.85it/s, loss=0.34, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  20%|██        | 81/400 [00:43<02:46,  1.91it/s, loss=0.34, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  20%|██        | 81/400 [00:43<02:46,  1.91it/s, loss=0.0021, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  20%|██        | 82/400 [00:44<02:43,  1.94it/s, loss=0.0021, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  20%|██        | 82/400 [00:44<02:43,  1.94it/s, loss=0.0953, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  21%|██        | 83/400 [00:44<02:40,  1.98it/s, loss=0.0953, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  21%|██        | 83/400 [00:44<02:40,  1.98it/s, loss=0.323, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  21%|██        | 84/400 [00:45<02:38,  1.99it/s, loss=0.323, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  21%|██        | 84/400 [00:45<02:38,  1.99it/s, loss=0.0596, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  21%|██▏       | 85/400 [00:45<02:36,  2.01it/s, loss=0.0596, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  21%|██▏       | 85/400 [00:45<02:36,  2.01it/s, loss=0.231, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  22%|██▏       | 86/400 [00:46<02:35,  2.02it/s, loss=0.231, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  22%|██▏       | 86/400 [00:46<02:35,  2.02it/s, loss=0.00739, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  22%|██▏       | 87/400 [00:46<02:34,  2.03it/s, loss=0.00739, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  22%|██▏       | 87/400 [00:46<02:34,  2.03it/s, loss=0.0268, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  22%|██▏       | 88/400 [00:47<02:33,  2.03it/s, loss=0.0268, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  22%|██▏       | 88/400 [00:47<02:33,  2.03it/s, loss=0.13, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  22%|██▏       | 89/400 [00:47<02:33,  2.03it/s, loss=0.13, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  22%|██▏       | 89/400 [00:47<02:33,  2.03it/s, loss=0.25, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  22%|██▎       | 90/400 [00:48<02:33,  2.02it/s, loss=0.25, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  22%|██▎       | 90/400 [00:48<02:33,  2.02it/s, loss=0.00393, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  23%|██▎       | 91/400 [00:48<02:32,  2.03it/s, loss=0.00393, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  23%|██▎       | 91/400 [00:48<02:32,  2.03it/s, loss=0.0994, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  23%|██▎       | 92/400 [00:49<02:31,  2.03it/s, loss=0.0994, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  23%|██▎       | 92/400 [00:49<02:31,  2.03it/s, loss=0.848, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  23%|██▎       | 93/400 [00:49<02:31,  2.03it/s, loss=0.848, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  23%|██▎       | 93/400 [00:49<02:31,  2.03it/s, loss=0.141, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  24%|██▎       | 94/400 [00:49<02:31,  2.03it/s, loss=0.141, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  24%|██▎       | 94/400 [00:50<02:31,  2.03it/s, loss=0.00233, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  24%|██▍       | 95/400 [00:50<02:35,  1.96it/s, loss=0.00233, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  24%|██▍       | 95/400 [00:50<02:35,  1.96it/s, loss=0.115, lr=2e-6]\u001b[0m\n",
      "\u001b[34mEnd epoch 4: train_avg_loss=0.15423133698488145\u001b[0m\n",
      "\u001b[34mEpoch: 5\u001b[0m\n",
      "\u001b[34mSteps:  24%|██▍       | 96/400 [00:51<03:11,  1.58it/s, loss=0.115, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  24%|██▍       | 96/400 [00:51<03:11,  1.58it/s, loss=0.0344, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  24%|██▍       | 97/400 [00:51<02:58,  1.70it/s, loss=0.0344, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  24%|██▍       | 97/400 [00:51<02:58,  1.70it/s, loss=0.00643, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  24%|██▍       | 98/400 [00:52<02:48,  1.79it/s, loss=0.00643, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  24%|██▍       | 98/400 [00:52<02:48,  1.79it/s, loss=0.184, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  25%|██▍       | 99/400 [00:52<02:41,  1.86it/s, loss=0.184, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  25%|██▍       | 99/400 [00:52<02:41,  1.86it/s, loss=0.0262, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  25%|██▌       | 100/400 [00:53<02:37,  1.91it/s, loss=0.0262, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  25%|██▌       | 100/400 [00:53<02:37,  1.91it/s, loss=0.0195, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  25%|██▌       | 101/400 [00:53<02:33,  1.95it/s, loss=0.0195, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  25%|██▌       | 101/400 [00:53<02:33,  1.95it/s, loss=0.00992, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  26%|██▌       | 102/400 [00:54<02:30,  1.98it/s, loss=0.00992, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  26%|██▌       | 102/400 [00:54<02:30,  1.98it/s, loss=0.241, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  26%|██▌       | 103/400 [00:54<02:29,  1.99it/s, loss=0.241, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  26%|██▌       | 103/400 [00:54<02:29,  1.99it/s, loss=0.202, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  26%|██▌       | 104/400 [00:55<02:27,  2.00it/s, loss=0.202, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  26%|██▌       | 104/400 [00:55<02:27,  2.00it/s, loss=0.238, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  26%|██▋       | 105/400 [00:55<02:25,  2.02it/s, loss=0.238, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  26%|██▋       | 105/400 [00:55<02:25,  2.02it/s, loss=0.0235, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  26%|██▋       | 106/400 [00:56<02:25,  2.02it/s, loss=0.0235, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  26%|██▋       | 106/400 [00:56<02:25,  2.02it/s, loss=0.00926, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  27%|██▋       | 107/400 [00:56<02:24,  2.03it/s, loss=0.00926, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  27%|██▋       | 107/400 [00:56<02:24,  2.03it/s, loss=0.21, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  27%|██▋       | 108/400 [00:57<02:24,  2.03it/s, loss=0.21, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  27%|██▋       | 108/400 [00:57<02:24,  2.03it/s, loss=0.0224, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  27%|██▋       | 109/400 [00:57<02:22,  2.04it/s, loss=0.0224, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  27%|██▋       | 109/400 [00:57<02:22,  2.04it/s, loss=0.00431, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  28%|██▊       | 110/400 [00:58<02:21,  2.04it/s, loss=0.00431, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  28%|██▊       | 110/400 [00:58<02:21,  2.04it/s, loss=0.00529, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  28%|██▊       | 111/400 [00:58<02:21,  2.04it/s, loss=0.00529, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  28%|██▊       | 111/400 [00:58<02:21,  2.04it/s, loss=0.0959, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  28%|██▊       | 112/400 [00:59<02:20,  2.04it/s, loss=0.0959, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  28%|██▊       | 112/400 [00:59<02:20,  2.04it/s, loss=0.143, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  28%|██▊       | 113/400 [00:59<02:20,  2.04it/s, loss=0.143, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  28%|██▊       | 113/400 [00:59<02:20,  2.04it/s, loss=0.00317, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  28%|██▊       | 114/400 [01:00<02:25,  1.97it/s, loss=0.00317, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  28%|██▊       | 114/400 [01:00<02:25,  1.97it/s, loss=0.463, lr=2e-6]\u001b[0m\n",
      "\u001b[34mEnd epoch 5: train_avg_loss=0.10215384613624529\u001b[0m\n",
      "\u001b[34mEpoch: 6\u001b[0m\n",
      "\u001b[34mSteps:  29%|██▉       | 115/400 [01:01<03:03,  1.55it/s, loss=0.463, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  29%|██▉       | 115/400 [01:01<03:03,  1.55it/s, loss=0.105, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  29%|██▉       | 116/400 [01:01<02:50,  1.67it/s, loss=0.105, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  29%|██▉       | 116/400 [01:01<02:50,  1.67it/s, loss=0.0197, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  29%|██▉       | 117/400 [01:02<02:40,  1.77it/s, loss=0.0197, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  29%|██▉       | 117/400 [01:02<02:40,  1.77it/s, loss=0.217, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  30%|██▉       | 118/400 [01:02<02:33,  1.84it/s, loss=0.217, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  30%|██▉       | 118/400 [01:02<02:33,  1.84it/s, loss=0.0445, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  30%|██▉       | 119/400 [01:03<02:28,  1.89it/s, loss=0.0445, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  30%|██▉       | 119/400 [01:03<02:28,  1.89it/s, loss=0.337, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  30%|███       | 120/400 [01:03<02:24,  1.94it/s, loss=0.337, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  30%|███       | 120/400 [01:03<02:24,  1.94it/s, loss=0.00736, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  30%|███       | 121/400 [01:04<02:21,  1.96it/s, loss=0.00736, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  30%|███       | 121/400 [01:04<02:21,  1.96it/s, loss=0.0137, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  30%|███       | 122/400 [01:04<02:20,  1.98it/s, loss=0.0137, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  30%|███       | 122/400 [01:04<02:20,  1.98it/s, loss=0.00406, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  31%|███       | 123/400 [01:05<02:17,  2.01it/s, loss=0.00406, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  31%|███       | 123/400 [01:05<02:17,  2.01it/s, loss=0.00694, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  31%|███       | 124/400 [01:05<02:17,  2.01it/s, loss=0.00694, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  31%|███       | 124/400 [01:05<02:17,  2.01it/s, loss=0.388, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  31%|███▏      | 125/400 [01:06<02:15,  2.02it/s, loss=0.388, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  31%|███▏      | 125/400 [01:06<02:15,  2.02it/s, loss=0.417, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  32%|███▏      | 126/400 [01:06<02:15,  2.03it/s, loss=0.417, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  32%|███▏      | 126/400 [01:06<02:15,  2.03it/s, loss=0.0602, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  32%|███▏      | 127/400 [01:07<02:14,  2.03it/s, loss=0.0602, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  32%|███▏      | 127/400 [01:07<02:14,  2.03it/s, loss=0.0241, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  32%|███▏      | 128/400 [01:07<02:14,  2.03it/s, loss=0.0241, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  32%|███▏      | 128/400 [01:07<02:14,  2.03it/s, loss=0.159, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  32%|███▏      | 129/400 [01:08<02:14,  2.02it/s, loss=0.159, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  32%|███▏      | 129/400 [01:08<02:14,  2.02it/s, loss=0.0459, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  32%|███▎      | 130/400 [01:08<02:13,  2.03it/s, loss=0.0459, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  32%|███▎      | 130/400 [01:08<02:13,  2.03it/s, loss=0.0972, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  33%|███▎      | 131/400 [01:09<02:12,  2.03it/s, loss=0.0972, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  33%|███▎      | 131/400 [01:09<02:12,  2.03it/s, loss=0.0219, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  33%|███▎      | 132/400 [01:09<02:12,  2.03it/s, loss=0.0219, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  33%|███▎      | 132/400 [01:09<02:12,  2.03it/s, loss=0.0516, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  33%|███▎      | 133/400 [01:10<02:16,  1.96it/s, loss=0.0516, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  33%|███▎      | 133/400 [01:10<02:16,  1.96it/s, loss=0.16, lr=2e-6]\u001b[0m\n",
      "\u001b[34mEnd epoch 6: train_avg_loss=0.11467795034772471\u001b[0m\n",
      "\u001b[34mEpoch: 7\u001b[0m\n",
      "\u001b[34mSteps:  34%|███▎      | 134/400 [01:11<02:51,  1.55it/s, loss=0.16, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  34%|███▎      | 134/400 [01:11<02:51,  1.55it/s, loss=0.0314, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  34%|███▍      | 135/400 [01:11<02:38,  1.68it/s, loss=0.0314, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  34%|███▍      | 135/400 [01:11<02:38,  1.68it/s, loss=0.6, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  34%|███▍      | 136/400 [01:12<02:29,  1.76it/s, loss=0.6, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  34%|███▍      | 136/400 [01:12<02:29,  1.76it/s, loss=0.941, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  34%|███▍      | 137/400 [01:12<02:22,  1.84it/s, loss=0.941, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  34%|███▍      | 137/400 [01:12<02:22,  1.84it/s, loss=0.0629, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  34%|███▍      | 138/400 [01:13<02:18,  1.90it/s, loss=0.0629, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  34%|███▍      | 138/400 [01:13<02:18,  1.90it/s, loss=0.0754, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  35%|███▍      | 139/400 [01:13<02:15,  1.93it/s, loss=0.0754, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  35%|███▍      | 139/400 [01:13<02:15,  1.93it/s, loss=0.00317, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  35%|███▌      | 140/400 [01:14<02:12,  1.96it/s, loss=0.00317, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  35%|███▌      | 140/400 [01:14<02:12,  1.96it/s, loss=0.109, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  35%|███▌      | 141/400 [01:14<02:10,  1.98it/s, loss=0.109, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  35%|███▌      | 141/400 [01:14<02:10,  1.98it/s, loss=0.00322, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  36%|███▌      | 142/400 [01:15<02:09,  2.00it/s, loss=0.00322, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  36%|███▌      | 142/400 [01:15<02:09,  2.00it/s, loss=0.27, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  36%|███▌      | 143/400 [01:15<02:08,  2.00it/s, loss=0.27, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  36%|███▌      | 143/400 [01:15<02:08,  2.00it/s, loss=0.0385, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  36%|███▌      | 144/400 [01:16<02:06,  2.02it/s, loss=0.0385, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  36%|███▌      | 144/400 [01:16<02:06,  2.02it/s, loss=0.692, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  36%|███▋      | 145/400 [01:16<02:06,  2.02it/s, loss=0.692, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  36%|███▋      | 145/400 [01:16<02:06,  2.02it/s, loss=0.163, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  36%|███▋      | 146/400 [01:17<02:05,  2.03it/s, loss=0.163, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  36%|███▋      | 146/400 [01:17<02:05,  2.03it/s, loss=0.685, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  37%|███▋      | 147/400 [01:17<02:04,  2.03it/s, loss=0.685, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  37%|███▋      | 147/400 [01:17<02:04,  2.03it/s, loss=0.153, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  37%|███▋      | 148/400 [01:18<02:04,  2.03it/s, loss=0.153, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  37%|███▋      | 148/400 [01:18<02:04,  2.03it/s, loss=0.00414, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  37%|███▋      | 149/400 [01:18<02:04,  2.02it/s, loss=0.00414, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  37%|███▋      | 149/400 [01:18<02:04,  2.02it/s, loss=0.329, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  38%|███▊      | 150/400 [01:19<02:02,  2.03it/s, loss=0.329, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  38%|███▊      | 150/400 [01:19<02:02,  2.03it/s, loss=0.047, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  38%|███▊      | 151/400 [01:19<02:02,  2.03it/s, loss=0.047, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  38%|███▊      | 151/400 [01:19<02:02,  2.03it/s, loss=0.0274, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  38%|███▊      | 152/400 [01:20<02:06,  1.96it/s, loss=0.0274, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  38%|███▊      | 152/400 [01:20<02:06,  1.96it/s, loss=0.0934, lr=2e-6]\u001b[0m\n",
      "\u001b[34mEnd epoch 7: train_avg_loss=0.22781970459771783\u001b[0m\n",
      "\u001b[34mEpoch: 8\u001b[0m\n",
      "\u001b[34mSteps:  38%|███▊      | 153/400 [01:21<02:40,  1.54it/s, loss=0.0934, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  38%|███▊      | 153/400 [01:21<02:40,  1.54it/s, loss=0.0134, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  38%|███▊      | 154/400 [01:21<02:27,  1.66it/s, loss=0.0134, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  38%|███▊      | 154/400 [01:21<02:27,  1.66it/s, loss=0.286, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  39%|███▉      | 155/400 [01:22<02:19,  1.76it/s, loss=0.286, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  39%|███▉      | 155/400 [01:22<02:19,  1.76it/s, loss=0.116, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  39%|███▉      | 156/400 [01:22<02:12,  1.84it/s, loss=0.116, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  39%|███▉      | 156/400 [01:22<02:12,  1.84it/s, loss=0.0096, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  39%|███▉      | 157/400 [01:23<02:08,  1.88it/s, loss=0.0096, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  39%|███▉      | 157/400 [01:23<02:08,  1.88it/s, loss=0.307, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  40%|███▉      | 158/400 [01:23<02:05,  1.93it/s, loss=0.307, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  40%|███▉      | 158/400 [01:23<02:05,  1.93it/s, loss=0.00879, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  40%|███▉      | 159/400 [01:23<02:02,  1.96it/s, loss=0.00879, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  40%|███▉      | 159/400 [01:24<02:02,  1.96it/s, loss=0.519, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  40%|████      | 160/400 [01:24<02:00,  1.98it/s, loss=0.519, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  40%|████      | 160/400 [01:24<02:00,  1.98it/s, loss=0.0666, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  40%|████      | 161/400 [01:24<01:59,  2.00it/s, loss=0.0666, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  40%|████      | 161/400 [01:25<01:59,  2.00it/s, loss=0.0218, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  40%|████      | 162/400 [01:25<01:58,  2.00it/s, loss=0.0218, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  40%|████      | 162/400 [01:25<01:58,  2.00it/s, loss=0.0198, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  41%|████      | 163/400 [01:25<01:57,  2.01it/s, loss=0.0198, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  41%|████      | 163/400 [01:25<01:57,  2.01it/s, loss=0.211, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  41%|████      | 164/400 [01:26<01:56,  2.03it/s, loss=0.211, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  41%|████      | 164/400 [01:26<01:56,  2.03it/s, loss=0.401, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  41%|████▏     | 165/400 [01:26<01:55,  2.03it/s, loss=0.401, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  41%|████▏     | 165/400 [01:26<01:55,  2.03it/s, loss=0.00802, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  42%|████▏     | 166/400 [01:27<01:54,  2.04it/s, loss=0.00802, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  42%|████▏     | 166/400 [01:27<01:54,  2.04it/s, loss=0.0975, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  42%|████▏     | 167/400 [01:27<01:54,  2.04it/s, loss=0.0975, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  42%|████▏     | 167/400 [01:27<01:54,  2.04it/s, loss=0.012, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  42%|████▏     | 168/400 [01:28<01:54,  2.03it/s, loss=0.012, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  42%|████▏     | 168/400 [01:28<01:54,  2.03it/s, loss=0.551, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  42%|████▏     | 169/400 [01:28<01:53,  2.04it/s, loss=0.551, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  42%|████▏     | 169/400 [01:28<01:53,  2.04it/s, loss=0.431, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  42%|████▎     | 170/400 [01:29<01:52,  2.04it/s, loss=0.431, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  42%|████▎     | 170/400 [01:29<01:52,  2.04it/s, loss=0.26, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  43%|████▎     | 171/400 [01:29<01:56,  1.96it/s, loss=0.26, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  43%|████▎     | 171/400 [01:29<01:56,  1.96it/s, loss=0.109, lr=2e-6]\u001b[0m\n",
      "\u001b[34mEnd epoch 8: train_avg_loss=0.1813829871580789\u001b[0m\n",
      "\u001b[34mEpoch: 9\u001b[0m\n",
      "\u001b[34mSteps:  43%|████▎     | 172/400 [01:30<02:26,  1.56it/s, loss=0.109, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  43%|████▎     | 172/400 [01:30<02:26,  1.56it/s, loss=0.00946, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  43%|████▎     | 173/400 [01:31<02:15,  1.68it/s, loss=0.00946, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  43%|████▎     | 173/400 [01:31<02:15,  1.68it/s, loss=0.128, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  44%|████▎     | 174/400 [01:31<02:08,  1.76it/s, loss=0.128, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  44%|████▎     | 174/400 [01:31<02:08,  1.76it/s, loss=0.0402, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  44%|████▍     | 175/400 [01:32<02:01,  1.85it/s, loss=0.0402, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  44%|████▍     | 175/400 [01:32<02:01,  1.85it/s, loss=0.396, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  44%|████▍     | 176/400 [01:32<01:58,  1.89it/s, loss=0.396, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  44%|████▍     | 176/400 [01:32<01:58,  1.89it/s, loss=0.453, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  44%|████▍     | 177/400 [01:33<01:55,  1.94it/s, loss=0.453, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  44%|████▍     | 177/400 [01:33<01:55,  1.94it/s, loss=0.189, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  44%|████▍     | 178/400 [01:33<01:52,  1.97it/s, loss=0.189, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  44%|████▍     | 178/400 [01:33<01:52,  1.97it/s, loss=0.00298, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  45%|████▍     | 179/400 [01:34<01:51,  1.98it/s, loss=0.00298, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  45%|████▍     | 179/400 [01:34<01:51,  1.98it/s, loss=0.421, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  45%|████▌     | 180/400 [01:34<01:49,  2.00it/s, loss=0.421, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  45%|████▌     | 180/400 [01:34<01:49,  2.00it/s, loss=0.0337, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  45%|████▌     | 181/400 [01:35<01:48,  2.02it/s, loss=0.0337, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  45%|████▌     | 181/400 [01:35<01:48,  2.02it/s, loss=0.0207, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  46%|████▌     | 182/400 [01:35<01:47,  2.02it/s, loss=0.0207, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  46%|████▌     | 182/400 [01:35<01:47,  2.02it/s, loss=0.113, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  46%|████▌     | 183/400 [01:36<01:46,  2.03it/s, loss=0.113, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  46%|████▌     | 183/400 [01:36<01:46,  2.03it/s, loss=0.00668, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  46%|████▌     | 184/400 [01:36<01:46,  2.02it/s, loss=0.00668, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  46%|████▌     | 184/400 [01:36<01:46,  2.02it/s, loss=0.305, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  46%|████▋     | 185/400 [01:37<01:45,  2.04it/s, loss=0.305, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  46%|████▋     | 185/400 [01:37<01:45,  2.04it/s, loss=0.0347, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  46%|████▋     | 186/400 [01:37<01:45,  2.03it/s, loss=0.0347, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  46%|████▋     | 186/400 [01:37<01:45,  2.03it/s, loss=0.0246, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  47%|████▋     | 187/400 [01:38<01:44,  2.03it/s, loss=0.0246, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  47%|████▋     | 187/400 [01:38<01:44,  2.03it/s, loss=0.0783, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  47%|████▋     | 188/400 [01:38<01:44,  2.04it/s, loss=0.0783, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  47%|████▋     | 188/400 [01:38<01:44,  2.04it/s, loss=0.407, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  47%|████▋     | 189/400 [01:39<01:43,  2.04it/s, loss=0.407, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  47%|████▋     | 189/400 [01:39<01:43,  2.04it/s, loss=0.014, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  48%|████▊     | 190/400 [01:39<01:47,  1.96it/s, loss=0.014, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  48%|████▊     | 190/400 [01:39<01:47,  1.96it/s, loss=0.194, lr=2e-6]\u001b[0m\n",
      "\u001b[34mEnd epoch 9: train_avg_loss=0.1510520665661285\u001b[0m\n",
      "\u001b[34mEpoch: 10\u001b[0m\n",
      "\u001b[34mSteps:  48%|████▊     | 191/400 [01:40<02:14,  1.55it/s, loss=0.194, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  48%|████▊     | 191/400 [01:40<02:14,  1.55it/s, loss=0.0627, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  48%|████▊     | 192/400 [01:41<02:04,  1.67it/s, loss=0.0627, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  48%|████▊     | 192/400 [01:41<02:04,  1.67it/s, loss=0.0674, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  48%|████▊     | 193/400 [01:41<01:57,  1.77it/s, loss=0.0674, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  48%|████▊     | 193/400 [01:41<01:57,  1.77it/s, loss=0.013, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  48%|████▊     | 194/400 [01:42<01:52,  1.83it/s, loss=0.013, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  48%|████▊     | 194/400 [01:42<01:52,  1.83it/s, loss=0.0821, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  49%|████▉     | 195/400 [01:42<01:48,  1.90it/s, loss=0.0821, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  49%|████▉     | 195/400 [01:42<01:48,  1.90it/s, loss=0.326, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  49%|████▉     | 196/400 [01:43<01:46,  1.92it/s, loss=0.326, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  49%|████▉     | 196/400 [01:43<01:46,  1.92it/s, loss=0.78, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  49%|████▉     | 197/400 [01:43<01:43,  1.97it/s, loss=0.78, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  49%|████▉     | 197/400 [01:43<01:43,  1.97it/s, loss=0.0124, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  50%|████▉     | 198/400 [01:44<01:41,  1.98it/s, loss=0.0124, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  50%|████▉     | 198/400 [01:44<01:41,  1.98it/s, loss=0.211, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  50%|████▉     | 199/400 [01:44<01:40,  2.00it/s, loss=0.211, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  50%|████▉     | 199/400 [01:44<01:40,  2.00it/s, loss=0.0212, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  50%|█████     | 200/400 [01:45<01:39,  2.01it/s, loss=0.0212, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  50%|█████     | 200/400 [01:45<01:39,  2.01it/s, loss=0.0255, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  50%|█████     | 201/400 [01:45<01:38,  2.02it/s, loss=0.0255, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  50%|█████     | 201/400 [01:45<01:38,  2.02it/s, loss=0.0748, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  50%|█████     | 202/400 [01:46<01:37,  2.02it/s, loss=0.0748, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  50%|█████     | 202/400 [01:46<01:37,  2.02it/s, loss=0.173, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  51%|█████     | 203/400 [01:46<01:37,  2.02it/s, loss=0.173, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  51%|█████     | 203/400 [01:46<01:37,  2.02it/s, loss=0.0473, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  51%|█████     | 204/400 [01:47<01:36,  2.03it/s, loss=0.0473, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  51%|█████     | 204/400 [01:47<01:36,  2.03it/s, loss=0.0242, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  51%|█████▏    | 205/400 [01:47<01:35,  2.04it/s, loss=0.0242, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  51%|█████▏    | 205/400 [01:47<01:35,  2.04it/s, loss=0.708, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  52%|█████▏    | 206/400 [01:48<01:35,  2.03it/s, loss=0.708, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  52%|█████▏    | 206/400 [01:48<01:35,  2.03it/s, loss=0.0117, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  52%|█████▏    | 207/400 [01:48<01:34,  2.03it/s, loss=0.0117, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  52%|█████▏    | 207/400 [01:48<01:34,  2.03it/s, loss=0.0852, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  52%|█████▏    | 208/400 [01:49<01:34,  2.04it/s, loss=0.0852, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  52%|█████▏    | 208/400 [01:49<01:34,  2.04it/s, loss=0.0187, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  52%|█████▏    | 209/400 [01:49<01:37,  1.97it/s, loss=0.0187, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  52%|█████▏    | 209/400 [01:49<01:37,  1.97it/s, loss=0.22, lr=2e-6]\u001b[0m\n",
      "\u001b[34mEnd epoch 10: train_avg_loss=0.15596007516509608\u001b[0m\n",
      "\u001b[34mEpoch: 11\u001b[0m\n",
      "\u001b[34mSteps:  52%|█████▎    | 210/400 [01:50<02:00,  1.58it/s, loss=0.22, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  52%|█████▎    | 210/400 [01:50<02:00,  1.58it/s, loss=0.331, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  53%|█████▎    | 211/400 [01:51<01:51,  1.69it/s, loss=0.331, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  53%|█████▎    | 211/400 [01:51<01:51,  1.69it/s, loss=0.208, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  53%|█████▎    | 212/400 [01:51<01:45,  1.79it/s, loss=0.208, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  53%|█████▎    | 212/400 [01:51<01:45,  1.79it/s, loss=0.0266, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  53%|█████▎    | 213/400 [01:52<01:40,  1.86it/s, loss=0.0266, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  53%|█████▎    | 213/400 [01:52<01:40,  1.86it/s, loss=0.0272, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  54%|█████▎    | 214/400 [01:52<01:37,  1.91it/s, loss=0.0272, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  54%|█████▎    | 214/400 [01:52<01:37,  1.91it/s, loss=0.0195, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  54%|█████▍    | 215/400 [01:53<01:35,  1.93it/s, loss=0.0195, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  54%|█████▍    | 215/400 [01:53<01:35,  1.93it/s, loss=0.00506, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  54%|█████▍    | 216/400 [01:53<01:33,  1.97it/s, loss=0.00506, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  54%|█████▍    | 216/400 [01:53<01:33,  1.97it/s, loss=0.00849, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  54%|█████▍    | 217/400 [01:54<01:31,  1.99it/s, loss=0.00849, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  54%|█████▍    | 217/400 [01:54<01:31,  1.99it/s, loss=0.00398, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  55%|█████▍    | 218/400 [01:54<01:30,  2.01it/s, loss=0.00398, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  55%|█████▍    | 218/400 [01:54<01:30,  2.01it/s, loss=0.0137, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  55%|█████▍    | 219/400 [01:54<01:29,  2.02it/s, loss=0.0137, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  55%|█████▍    | 219/400 [01:55<01:29,  2.02it/s, loss=0.162, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  55%|█████▌    | 220/400 [01:55<01:29,  2.01it/s, loss=0.162, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  55%|█████▌    | 220/400 [01:55<01:29,  2.01it/s, loss=0.00329, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  55%|█████▌    | 221/400 [01:55<01:28,  2.03it/s, loss=0.00329, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  55%|█████▌    | 221/400 [01:56<01:28,  2.03it/s, loss=0.0871, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  56%|█████▌    | 222/400 [01:56<01:27,  2.03it/s, loss=0.0871, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  56%|█████▌    | 222/400 [01:56<01:27,  2.03it/s, loss=0.00536, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  56%|█████▌    | 223/400 [01:56<01:27,  2.03it/s, loss=0.00536, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  56%|█████▌    | 223/400 [01:56<01:27,  2.03it/s, loss=0.141, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  56%|█████▌    | 224/400 [01:57<01:26,  2.04it/s, loss=0.141, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  56%|█████▌    | 224/400 [01:57<01:26,  2.04it/s, loss=0.0471, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  56%|█████▋    | 225/400 [01:57<01:25,  2.04it/s, loss=0.0471, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  56%|█████▋    | 225/400 [01:57<01:25,  2.04it/s, loss=0.192, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  56%|█████▋    | 226/400 [01:58<01:25,  2.03it/s, loss=0.192, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  56%|█████▋    | 226/400 [01:58<01:25,  2.03it/s, loss=0.0937, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  57%|█████▋    | 227/400 [01:58<01:25,  2.03it/s, loss=0.0937, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  57%|█████▋    | 227/400 [01:58<01:25,  2.03it/s, loss=0.288, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  57%|█████▋    | 228/400 [01:59<01:27,  1.96it/s, loss=0.288, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  57%|█████▋    | 228/400 [01:59<01:27,  1.96it/s, loss=0.0268, lr=2e-6]\u001b[0m\n",
      "\u001b[34mEnd epoch 11: train_avg_loss=0.08897195246658827\u001b[0m\n",
      "\u001b[34mEpoch: 12\u001b[0m\n",
      "\u001b[34mSteps:  57%|█████▋    | 229/400 [02:00<01:47,  1.59it/s, loss=0.0268, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  57%|█████▋    | 229/400 [02:00<01:47,  1.59it/s, loss=0.163, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  57%|█████▊    | 230/400 [02:00<01:39,  1.70it/s, loss=0.163, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  57%|█████▊    | 230/400 [02:00<01:39,  1.70it/s, loss=0.018, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  58%|█████▊    | 231/400 [02:01<01:34,  1.79it/s, loss=0.018, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  58%|█████▊    | 231/400 [02:01<01:34,  1.79it/s, loss=0.0953, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  58%|█████▊    | 232/400 [02:01<01:30,  1.86it/s, loss=0.0953, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  58%|█████▊    | 232/400 [02:01<01:30,  1.86it/s, loss=0.23, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  58%|█████▊    | 233/400 [02:02<01:27,  1.91it/s, loss=0.23, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  58%|█████▊    | 233/400 [02:02<01:27,  1.91it/s, loss=0.0168, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  58%|█████▊    | 234/400 [02:02<01:25,  1.95it/s, loss=0.0168, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  58%|█████▊    | 234/400 [02:02<01:25,  1.95it/s, loss=0.00825, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  59%|█████▉    | 235/400 [02:03<01:23,  1.98it/s, loss=0.00825, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  59%|█████▉    | 235/400 [02:03<01:23,  1.98it/s, loss=0.022, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  59%|█████▉    | 236/400 [02:03<01:22,  2.00it/s, loss=0.022, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  59%|█████▉    | 236/400 [02:03<01:22,  2.00it/s, loss=0.037, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  59%|█████▉    | 237/400 [02:04<01:21,  2.00it/s, loss=0.037, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  59%|█████▉    | 237/400 [02:04<01:21,  2.00it/s, loss=0.19, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  60%|█████▉    | 238/400 [02:04<01:20,  2.00it/s, loss=0.19, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  60%|█████▉    | 238/400 [02:04<01:20,  2.00it/s, loss=0.0137, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  60%|█████▉    | 239/400 [02:05<01:19,  2.02it/s, loss=0.0137, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  60%|█████▉    | 239/400 [02:05<01:19,  2.02it/s, loss=0.00791, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  60%|██████    | 240/400 [02:05<01:18,  2.03it/s, loss=0.00791, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  60%|██████    | 240/400 [02:05<01:18,  2.03it/s, loss=0.0168, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  60%|██████    | 241/400 [02:06<01:18,  2.03it/s, loss=0.0168, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  60%|██████    | 241/400 [02:06<01:18,  2.03it/s, loss=0.0303, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  60%|██████    | 242/400 [02:06<01:17,  2.04it/s, loss=0.0303, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  60%|██████    | 242/400 [02:06<01:17,  2.04it/s, loss=0.142, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  61%|██████    | 243/400 [02:07<01:17,  2.03it/s, loss=0.142, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  61%|██████    | 243/400 [02:07<01:17,  2.03it/s, loss=0.0389, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  61%|██████    | 244/400 [02:07<01:16,  2.04it/s, loss=0.0389, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  61%|██████    | 244/400 [02:07<01:16,  2.04it/s, loss=0.0566, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  61%|██████▏   | 245/400 [02:08<01:16,  2.02it/s, loss=0.0566, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  61%|██████▏   | 245/400 [02:08<01:16,  2.02it/s, loss=0.00558, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  62%|██████▏   | 246/400 [02:08<01:16,  2.03it/s, loss=0.00558, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  62%|██████▏   | 246/400 [02:08<01:16,  2.03it/s, loss=0.0158, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  62%|██████▏   | 247/400 [02:09<01:18,  1.96it/s, loss=0.0158, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  62%|██████▏   | 247/400 [02:09<01:18,  1.96it/s, loss=0.313, lr=2e-6]\u001b[0m\n",
      "\u001b[34mEnd epoch 12: train_avg_loss=0.07476258317106649\u001b[0m\n",
      "\u001b[34mEpoch: 13\u001b[0m\n",
      "\u001b[34mSteps:  62%|██████▏   | 248/400 [02:10<01:36,  1.57it/s, loss=0.313, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  62%|██████▏   | 248/400 [02:10<01:36,  1.57it/s, loss=0.0316, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  62%|██████▏   | 249/400 [02:10<01:29,  1.69it/s, loss=0.0316, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  62%|██████▏   | 249/400 [02:10<01:29,  1.69it/s, loss=0.7, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  62%|██████▎   | 250/400 [02:11<01:24,  1.78it/s, loss=0.7, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  62%|██████▎   | 250/400 [02:11<01:24,  1.78it/s, loss=0.357, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  63%|██████▎   | 251/400 [02:11<01:20,  1.85it/s, loss=0.357, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  63%|██████▎   | 251/400 [02:11<01:20,  1.85it/s, loss=0.0893, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  63%|██████▎   | 252/400 [02:12<01:17,  1.90it/s, loss=0.0893, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  63%|██████▎   | 252/400 [02:12<01:17,  1.90it/s, loss=0.307, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  63%|██████▎   | 253/400 [02:12<01:15,  1.94it/s, loss=0.307, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  63%|██████▎   | 253/400 [02:12<01:15,  1.94it/s, loss=0.0315, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  64%|██████▎   | 254/400 [02:13<01:14,  1.97it/s, loss=0.0315, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  64%|██████▎   | 254/400 [02:13<01:14,  1.97it/s, loss=0.00474, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  64%|██████▍   | 255/400 [02:13<01:12,  1.99it/s, loss=0.00474, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  64%|██████▍   | 255/400 [02:13<01:12,  1.99it/s, loss=0.321, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  64%|██████▍   | 256/400 [02:14<01:11,  2.00it/s, loss=0.321, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  64%|██████▍   | 256/400 [02:14<01:11,  2.00it/s, loss=0.00472, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  64%|██████▍   | 257/400 [02:14<01:10,  2.01it/s, loss=0.00472, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  64%|██████▍   | 257/400 [02:14<01:10,  2.01it/s, loss=0.0707, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  64%|██████▍   | 258/400 [02:15<01:10,  2.02it/s, loss=0.0707, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  64%|██████▍   | 258/400 [02:15<01:10,  2.02it/s, loss=0.45, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  65%|██████▍   | 259/400 [02:15<01:09,  2.02it/s, loss=0.45, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  65%|██████▍   | 259/400 [02:15<01:09,  2.02it/s, loss=0.0667, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  65%|██████▌   | 260/400 [02:16<01:08,  2.03it/s, loss=0.0667, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  65%|██████▌   | 260/400 [02:16<01:08,  2.03it/s, loss=0.0278, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  65%|██████▌   | 261/400 [02:16<01:08,  2.03it/s, loss=0.0278, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  65%|██████▌   | 261/400 [02:16<01:08,  2.03it/s, loss=0.0769, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  66%|██████▌   | 262/400 [02:17<01:08,  2.02it/s, loss=0.0769, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  66%|██████▌   | 262/400 [02:17<01:08,  2.02it/s, loss=0.326, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  66%|██████▌   | 263/400 [02:17<01:07,  2.04it/s, loss=0.326, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  66%|██████▌   | 263/400 [02:17<01:07,  2.04it/s, loss=0.0249, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  66%|██████▌   | 264/400 [02:18<01:06,  2.04it/s, loss=0.0249, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  66%|██████▌   | 264/400 [02:18<01:06,  2.04it/s, loss=0.136, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  66%|██████▋   | 265/400 [02:18<01:06,  2.04it/s, loss=0.136, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  66%|██████▋   | 265/400 [02:18<01:06,  2.04it/s, loss=0.00218, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  66%|██████▋   | 266/400 [02:19<01:08,  1.96it/s, loss=0.00218, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  66%|██████▋   | 266/400 [02:19<01:08,  1.96it/s, loss=0.0198, lr=2e-6]\u001b[0m\n",
      "\u001b[34mEnd epoch 13: train_avg_loss=0.16036815288182543\u001b[0m\n",
      "\u001b[34mEpoch: 14\u001b[0m\n",
      "\u001b[34mSteps:  67%|██████▋   | 267/400 [02:20<01:24,  1.58it/s, loss=0.0198, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  67%|██████▋   | 267/400 [02:20<01:24,  1.58it/s, loss=0.0297, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  67%|██████▋   | 268/400 [02:20<01:18,  1.69it/s, loss=0.0297, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  67%|██████▋   | 268/400 [02:20<01:18,  1.69it/s, loss=0.258, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  67%|██████▋   | 269/400 [02:21<01:13,  1.78it/s, loss=0.258, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  67%|██████▋   | 269/400 [02:21<01:13,  1.78it/s, loss=0.0047, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  68%|██████▊   | 270/400 [02:21<01:10,  1.85it/s, loss=0.0047, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  68%|██████▊   | 270/400 [02:21<01:10,  1.85it/s, loss=0.451, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  68%|██████▊   | 271/400 [02:22<01:07,  1.90it/s, loss=0.451, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  68%|██████▊   | 271/400 [02:22<01:07,  1.90it/s, loss=0.0339, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  68%|██████▊   | 272/400 [02:22<01:06,  1.94it/s, loss=0.0339, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  68%|██████▊   | 272/400 [02:22<01:06,  1.94it/s, loss=0.248, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  68%|██████▊   | 273/400 [02:22<01:04,  1.97it/s, loss=0.248, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  68%|██████▊   | 273/400 [02:23<01:04,  1.97it/s, loss=0.373, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  68%|██████▊   | 274/400 [02:23<01:09,  1.81it/s, loss=0.373, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  68%|██████▊   | 274/400 [02:23<01:09,  1.81it/s, loss=0.269, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  69%|██████▉   | 275/400 [02:24<01:06,  1.87it/s, loss=0.269, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  69%|██████▉   | 275/400 [02:24<01:06,  1.87it/s, loss=0.382, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  69%|██████▉   | 276/400 [02:24<01:04,  1.92it/s, loss=0.382, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  69%|██████▉   | 276/400 [02:24<01:04,  1.92it/s, loss=0.0146, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  69%|██████▉   | 277/400 [02:25<01:02,  1.96it/s, loss=0.0146, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  69%|██████▉   | 277/400 [02:25<01:02,  1.96it/s, loss=0.0523, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  70%|██████▉   | 278/400 [02:25<01:01,  1.98it/s, loss=0.0523, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  70%|██████▉   | 278/400 [02:25<01:01,  1.98it/s, loss=0.398, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  70%|██████▉   | 279/400 [02:26<01:00,  1.99it/s, loss=0.398, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  70%|██████▉   | 279/400 [02:26<01:00,  1.99it/s, loss=0.013, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  70%|███████   | 280/400 [02:26<00:59,  2.01it/s, loss=0.013, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  70%|███████   | 280/400 [02:26<00:59,  2.01it/s, loss=0.19, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  70%|███████   | 281/400 [02:27<00:59,  2.01it/s, loss=0.19, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  70%|███████   | 281/400 [02:27<00:59,  2.01it/s, loss=0.00703, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  70%|███████   | 282/400 [02:27<00:58,  2.02it/s, loss=0.00703, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  70%|███████   | 282/400 [02:27<00:58,  2.02it/s, loss=0.0715, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  71%|███████   | 283/400 [02:28<00:57,  2.03it/s, loss=0.0715, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  71%|███████   | 283/400 [02:28<00:57,  2.03it/s, loss=0.462, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  71%|███████   | 284/400 [02:28<00:57,  2.03it/s, loss=0.462, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  71%|███████   | 284/400 [02:28<00:57,  2.03it/s, loss=0.0206, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  71%|███████▏  | 285/400 [02:29<00:58,  1.96it/s, loss=0.0206, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  71%|███████▏  | 285/400 [02:29<00:58,  1.96it/s, loss=0.0359, lr=2e-6]\u001b[0m\n",
      "\u001b[34mEnd epoch 14: train_avg_loss=0.17446784273182092\u001b[0m\n",
      "\u001b[34mEpoch: 15\u001b[0m\n",
      "\u001b[34mSteps:  72%|███████▏  | 286/400 [02:30<01:13,  1.56it/s, loss=0.0359, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  72%|███████▏  | 286/400 [02:30<01:13,  1.56it/s, loss=0.502, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  72%|███████▏  | 287/400 [02:30<01:07,  1.67it/s, loss=0.502, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  72%|███████▏  | 287/400 [02:30<01:07,  1.67it/s, loss=0.322, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  72%|███████▏  | 288/400 [02:31<01:03,  1.77it/s, loss=0.322, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  72%|███████▏  | 288/400 [02:31<01:03,  1.77it/s, loss=0.172, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  72%|███████▏  | 289/400 [02:31<01:00,  1.84it/s, loss=0.172, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  72%|███████▏  | 289/400 [02:31<01:00,  1.84it/s, loss=0.117, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  72%|███████▎  | 290/400 [02:32<00:57,  1.90it/s, loss=0.117, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  72%|███████▎  | 290/400 [02:32<00:57,  1.90it/s, loss=0.156, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  73%|███████▎  | 291/400 [02:32<00:56,  1.94it/s, loss=0.156, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  73%|███████▎  | 291/400 [02:32<00:56,  1.94it/s, loss=0.0499, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  73%|███████▎  | 292/400 [02:33<00:55,  1.96it/s, loss=0.0499, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  73%|███████▎  | 292/400 [02:33<00:55,  1.96it/s, loss=0.377, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  73%|███████▎  | 293/400 [02:33<00:53,  1.98it/s, loss=0.377, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  73%|███████▎  | 293/400 [02:33<00:53,  1.98it/s, loss=0.0495, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  74%|███████▎  | 294/400 [02:33<00:52,  2.00it/s, loss=0.0495, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  74%|███████▎  | 294/400 [02:34<00:52,  2.00it/s, loss=0.00939, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  74%|███████▍  | 295/400 [02:34<00:52,  2.01it/s, loss=0.00939, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  74%|███████▍  | 295/400 [02:34<00:52,  2.01it/s, loss=0.0198, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  74%|███████▍  | 296/400 [02:34<00:51,  2.02it/s, loss=0.0198, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  74%|███████▍  | 296/400 [02:34<00:51,  2.02it/s, loss=0.0453, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  74%|███████▍  | 297/400 [02:35<00:50,  2.02it/s, loss=0.0453, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  74%|███████▍  | 297/400 [02:35<00:50,  2.02it/s, loss=0.0224, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  74%|███████▍  | 298/400 [02:35<00:50,  2.03it/s, loss=0.0224, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  74%|███████▍  | 298/400 [02:35<00:50,  2.03it/s, loss=0.327, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  75%|███████▍  | 299/400 [02:36<00:49,  2.03it/s, loss=0.327, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  75%|███████▍  | 299/400 [02:36<00:49,  2.03it/s, loss=0.00543, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  75%|███████▌  | 300/400 [02:36<00:49,  2.04it/s, loss=0.00543, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  75%|███████▌  | 300/400 [02:36<00:49,  2.04it/s, loss=0.659, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  75%|███████▌  | 301/400 [02:37<00:48,  2.04it/s, loss=0.659, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  75%|███████▌  | 301/400 [02:37<00:48,  2.04it/s, loss=0.391, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  76%|███████▌  | 302/400 [02:37<00:48,  2.02it/s, loss=0.391, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  76%|███████▌  | 302/400 [02:37<00:48,  2.02it/s, loss=0.0153, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  76%|███████▌  | 303/400 [02:38<00:47,  2.03it/s, loss=0.0153, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  76%|███████▌  | 303/400 [02:38<00:47,  2.03it/s, loss=0.558, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  76%|███████▌  | 304/400 [02:38<00:49,  1.95it/s, loss=0.558, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  76%|███████▌  | 304/400 [02:38<00:49,  1.95it/s, loss=0.00931, lr=2e-6]\u001b[0m\n",
      "\u001b[34mEnd epoch 15: train_avg_loss=0.20045093079342655\u001b[0m\n",
      "\u001b[34mEpoch: 16\u001b[0m\n",
      "\u001b[34mSteps:  76%|███████▋  | 305/400 [02:39<01:00,  1.56it/s, loss=0.00931, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  76%|███████▋  | 305/400 [02:39<01:00,  1.56it/s, loss=0.234, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  76%|███████▋  | 306/400 [02:40<00:56,  1.68it/s, loss=0.234, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  76%|███████▋  | 306/400 [02:40<00:56,  1.68it/s, loss=0.00431, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  77%|███████▋  | 307/400 [02:40<00:52,  1.77it/s, loss=0.00431, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  77%|███████▋  | 307/400 [02:40<00:52,  1.77it/s, loss=0.069, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  77%|███████▋  | 308/400 [02:41<00:49,  1.84it/s, loss=0.069, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  77%|███████▋  | 308/400 [02:41<00:49,  1.84it/s, loss=0.167, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  77%|███████▋  | 309/400 [02:41<00:47,  1.90it/s, loss=0.167, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  77%|███████▋  | 309/400 [02:41<00:47,  1.90it/s, loss=0.509, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  78%|███████▊  | 310/400 [02:42<00:46,  1.94it/s, loss=0.509, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  78%|███████▊  | 310/400 [02:42<00:46,  1.94it/s, loss=0.0105, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  78%|███████▊  | 311/400 [02:42<00:45,  1.95it/s, loss=0.0105, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  78%|███████▊  | 311/400 [02:42<00:45,  1.95it/s, loss=0.218, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  78%|███████▊  | 312/400 [02:43<00:44,  1.98it/s, loss=0.218, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  78%|███████▊  | 312/400 [02:43<00:44,  1.98it/s, loss=0.465, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  78%|███████▊  | 313/400 [02:43<00:43,  1.98it/s, loss=0.465, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  78%|███████▊  | 313/400 [02:43<00:43,  1.98it/s, loss=0.226, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  78%|███████▊  | 314/400 [02:44<00:43,  2.00it/s, loss=0.226, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  78%|███████▊  | 314/400 [02:44<00:43,  2.00it/s, loss=0.312, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  79%|███████▉  | 315/400 [02:44<00:42,  2.00it/s, loss=0.312, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  79%|███████▉  | 315/400 [02:44<00:42,  2.00it/s, loss=0.774, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  79%|███████▉  | 316/400 [02:45<00:41,  2.01it/s, loss=0.774, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  79%|███████▉  | 316/400 [02:45<00:41,  2.01it/s, loss=0.00227, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  79%|███████▉  | 317/400 [02:45<00:41,  2.02it/s, loss=0.00227, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  79%|███████▉  | 317/400 [02:45<00:41,  2.02it/s, loss=0.136, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  80%|███████▉  | 318/400 [02:46<00:40,  2.02it/s, loss=0.136, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  80%|███████▉  | 318/400 [02:46<00:40,  2.02it/s, loss=0.00292, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  80%|███████▉  | 319/400 [02:46<00:40,  2.01it/s, loss=0.00292, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  80%|███████▉  | 319/400 [02:46<00:40,  2.01it/s, loss=0.00496, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  80%|████████  | 320/400 [02:47<00:39,  2.02it/s, loss=0.00496, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  80%|████████  | 320/400 [02:47<00:39,  2.02it/s, loss=0.376, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  80%|████████  | 321/400 [02:47<00:39,  2.02it/s, loss=0.376, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  80%|████████  | 321/400 [02:47<00:39,  2.02it/s, loss=0.0158, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  80%|████████  | 322/400 [02:48<00:38,  2.03it/s, loss=0.0158, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  80%|████████  | 322/400 [02:48<00:38,  2.03it/s, loss=0.0559, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  81%|████████  | 323/400 [02:48<00:39,  1.95it/s, loss=0.0559, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  81%|████████  | 323/400 [02:48<00:39,  1.95it/s, loss=0.0386, lr=2e-6]\u001b[0m\n",
      "\u001b[34mEnd epoch 16: train_avg_loss=0.19054604757969318\u001b[0m\n",
      "\u001b[34mEpoch: 17\u001b[0m\n",
      "\u001b[34mSteps:  81%|████████  | 324/400 [02:49<00:48,  1.56it/s, loss=0.0386, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  81%|████████  | 324/400 [02:49<00:48,  1.56it/s, loss=0.0516, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  81%|████████▏ | 325/400 [02:50<00:44,  1.67it/s, loss=0.0516, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  81%|████████▏ | 325/400 [02:50<00:44,  1.67it/s, loss=0.0675, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  82%|████████▏ | 326/400 [02:50<00:42,  1.76it/s, loss=0.0675, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  82%|████████▏ | 326/400 [02:50<00:42,  1.76it/s, loss=0.136, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  82%|████████▏ | 327/400 [02:51<00:39,  1.84it/s, loss=0.136, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  82%|████████▏ | 327/400 [02:51<00:39,  1.84it/s, loss=0.231, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  82%|████████▏ | 328/400 [02:51<00:38,  1.89it/s, loss=0.231, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  82%|████████▏ | 328/400 [02:51<00:38,  1.89it/s, loss=0.203, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  82%|████████▏ | 329/400 [02:52<00:37,  1.91it/s, loss=0.203, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  82%|████████▏ | 329/400 [02:52<00:37,  1.91it/s, loss=0.0286, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  82%|████████▎ | 330/400 [02:52<00:36,  1.94it/s, loss=0.0286, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  82%|████████▎ | 330/400 [02:52<00:36,  1.94it/s, loss=0.031, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  83%|████████▎ | 331/400 [02:53<00:35,  1.97it/s, loss=0.031, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  83%|████████▎ | 331/400 [02:53<00:35,  1.97it/s, loss=0.00632, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  83%|████████▎ | 332/400 [02:53<00:34,  1.98it/s, loss=0.00632, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  83%|████████▎ | 332/400 [02:53<00:34,  1.98it/s, loss=0.00702, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  83%|████████▎ | 333/400 [02:54<00:33,  2.00it/s, loss=0.00702, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  83%|████████▎ | 333/400 [02:54<00:33,  2.00it/s, loss=0.0653, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  84%|████████▎ | 334/400 [02:54<00:33,  1.99it/s, loss=0.0653, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  84%|████████▎ | 334/400 [02:54<00:33,  1.99it/s, loss=0.224, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  84%|████████▍ | 335/400 [02:55<00:32,  1.99it/s, loss=0.224, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  84%|████████▍ | 335/400 [02:55<00:32,  1.99it/s, loss=0.0516, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  84%|████████▍ | 336/400 [02:55<00:31,  2.01it/s, loss=0.0516, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  84%|████████▍ | 336/400 [02:55<00:31,  2.01it/s, loss=0.00207, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  84%|████████▍ | 337/400 [02:56<00:31,  2.01it/s, loss=0.00207, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  84%|████████▍ | 337/400 [02:56<00:31,  2.01it/s, loss=0.00267, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  84%|████████▍ | 338/400 [02:56<00:30,  2.01it/s, loss=0.00267, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  84%|████████▍ | 338/400 [02:56<00:30,  2.01it/s, loss=0.335, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  85%|████████▍ | 339/400 [02:57<00:30,  2.02it/s, loss=0.335, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  85%|████████▍ | 339/400 [02:57<00:30,  2.02it/s, loss=0.136, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  85%|████████▌ | 340/400 [02:57<00:29,  2.02it/s, loss=0.136, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  85%|████████▌ | 340/400 [02:57<00:29,  2.02it/s, loss=0.0132, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  85%|████████▌ | 341/400 [02:58<00:29,  2.02it/s, loss=0.0132, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  85%|████████▌ | 341/400 [02:58<00:29,  2.02it/s, loss=0.137, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  86%|████████▌ | 342/400 [02:58<00:29,  1.95it/s, loss=0.137, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  86%|████████▌ | 342/400 [02:58<00:29,  1.95it/s, loss=0.142, lr=2e-6]\u001b[0m\n",
      "\u001b[34mEnd epoch 17: train_avg_loss=0.09854754709981774\u001b[0m\n",
      "\u001b[34mEpoch: 18\u001b[0m\n",
      "\u001b[34mSteps:  86%|████████▌ | 343/400 [02:59<00:36,  1.57it/s, loss=0.142, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  86%|████████▌ | 343/400 [02:59<00:36,  1.57it/s, loss=0.0056, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  86%|████████▌ | 344/400 [03:00<00:33,  1.69it/s, loss=0.0056, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  86%|████████▌ | 344/400 [03:00<00:33,  1.69it/s, loss=0.25, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  86%|████████▋ | 345/400 [03:00<00:30,  1.78it/s, loss=0.25, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  86%|████████▋ | 345/400 [03:00<00:30,  1.78it/s, loss=0.175, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  86%|████████▋ | 346/400 [03:01<00:29,  1.85it/s, loss=0.175, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  86%|████████▋ | 346/400 [03:01<00:29,  1.85it/s, loss=0.0936, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  87%|████████▋ | 347/400 [03:01<00:27,  1.90it/s, loss=0.0936, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  87%|████████▋ | 347/400 [03:01<00:27,  1.90it/s, loss=0.298, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  87%|████████▋ | 348/400 [03:02<00:26,  1.93it/s, loss=0.298, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  87%|████████▋ | 348/400 [03:02<00:26,  1.93it/s, loss=0.174, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  87%|████████▋ | 349/400 [03:02<00:25,  1.97it/s, loss=0.174, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  87%|████████▋ | 349/400 [03:02<00:25,  1.97it/s, loss=0.458, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  88%|████████▊ | 350/400 [03:03<00:25,  1.99it/s, loss=0.458, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  88%|████████▊ | 350/400 [03:03<00:25,  1.99it/s, loss=0.00349, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  88%|████████▊ | 351/400 [03:03<00:24,  2.00it/s, loss=0.00349, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  88%|████████▊ | 351/400 [03:03<00:24,  2.00it/s, loss=0.00449, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  88%|████████▊ | 352/400 [03:04<00:23,  2.01it/s, loss=0.00449, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  88%|████████▊ | 352/400 [03:04<00:23,  2.01it/s, loss=0.288, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  88%|████████▊ | 353/400 [03:04<00:23,  2.02it/s, loss=0.288, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  88%|████████▊ | 353/400 [03:04<00:23,  2.02it/s, loss=0.00422, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  88%|████████▊ | 354/400 [03:05<00:22,  2.02it/s, loss=0.00422, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  88%|████████▊ | 354/400 [03:05<00:22,  2.02it/s, loss=0.168, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  89%|████████▉ | 355/400 [03:05<00:22,  2.02it/s, loss=0.168, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  89%|████████▉ | 355/400 [03:05<00:22,  2.02it/s, loss=0.0089, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  89%|████████▉ | 356/400 [03:06<00:21,  2.01it/s, loss=0.0089, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  89%|████████▉ | 356/400 [03:06<00:21,  2.01it/s, loss=0.0483, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  89%|████████▉ | 357/400 [03:06<00:21,  2.01it/s, loss=0.0483, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  89%|████████▉ | 357/400 [03:06<00:21,  2.01it/s, loss=0.00443, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  90%|████████▉ | 358/400 [03:07<00:20,  2.02it/s, loss=0.00443, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  90%|████████▉ | 358/400 [03:07<00:20,  2.02it/s, loss=0.0398, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  90%|████████▉ | 359/400 [03:07<00:20,  2.02it/s, loss=0.0398, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  90%|████████▉ | 359/400 [03:07<00:20,  2.02it/s, loss=0.119, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  90%|█████████ | 360/400 [03:08<00:19,  2.02it/s, loss=0.119, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  90%|█████████ | 360/400 [03:08<00:19,  2.02it/s, loss=0.32, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  90%|█████████ | 361/400 [03:08<00:20,  1.92it/s, loss=0.32, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  90%|█████████ | 361/400 [03:08<00:20,  1.92it/s, loss=0.754, lr=2e-6]\u001b[0m\n",
      "\u001b[34mEnd epoch 18: train_avg_loss=0.16929074381723216\u001b[0m\n",
      "\u001b[34mEpoch: 19\u001b[0m\n",
      "\u001b[34mSteps:  90%|█████████ | 362/400 [03:09<00:24,  1.54it/s, loss=0.754, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  90%|█████████ | 362/400 [03:09<00:24,  1.54it/s, loss=0.23, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  91%|█████████ | 363/400 [03:10<00:22,  1.66it/s, loss=0.23, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  91%|█████████ | 363/400 [03:10<00:22,  1.66it/s, loss=0.0148, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  91%|█████████ | 364/400 [03:10<00:20,  1.76it/s, loss=0.0148, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  91%|█████████ | 364/400 [03:10<00:20,  1.76it/s, loss=0.149, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  91%|█████████▏| 365/400 [03:11<00:19,  1.83it/s, loss=0.149, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  91%|█████████▏| 365/400 [03:11<00:19,  1.83it/s, loss=0.022, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  92%|█████████▏| 366/400 [03:11<00:17,  1.89it/s, loss=0.022, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  92%|█████████▏| 366/400 [03:11<00:17,  1.89it/s, loss=0.00237, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  92%|█████████▏| 367/400 [03:12<00:17,  1.93it/s, loss=0.00237, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  92%|█████████▏| 367/400 [03:12<00:17,  1.93it/s, loss=0.0154, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  92%|█████████▏| 368/400 [03:12<00:16,  1.96it/s, loss=0.0154, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  92%|█████████▏| 368/400 [03:12<00:16,  1.96it/s, loss=0.00766, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  92%|█████████▏| 369/400 [03:13<00:15,  1.97it/s, loss=0.00766, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  92%|█████████▏| 369/400 [03:13<00:15,  1.97it/s, loss=0.0928, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  92%|█████████▎| 370/400 [03:13<00:15,  1.99it/s, loss=0.0928, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  92%|█████████▎| 370/400 [03:13<00:15,  1.99it/s, loss=0.0128, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  93%|█████████▎| 371/400 [03:14<00:14,  2.00it/s, loss=0.0128, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  93%|█████████▎| 371/400 [03:14<00:14,  2.00it/s, loss=0.00262, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  93%|█████████▎| 372/400 [03:14<00:13,  2.01it/s, loss=0.00262, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  93%|█████████▎| 372/400 [03:14<00:13,  2.01it/s, loss=0.0053, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  93%|█████████▎| 373/400 [03:15<00:13,  2.01it/s, loss=0.0053, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  93%|█████████▎| 373/400 [03:15<00:13,  2.01it/s, loss=0.0304, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  94%|█████████▎| 374/400 [03:15<00:12,  2.02it/s, loss=0.0304, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  94%|█████████▎| 374/400 [03:15<00:12,  2.02it/s, loss=0.0213, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  94%|█████████▍| 375/400 [03:16<00:12,  2.02it/s, loss=0.0213, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  94%|█████████▍| 375/400 [03:16<00:12,  2.02it/s, loss=0.545, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  94%|█████████▍| 376/400 [03:16<00:11,  2.02it/s, loss=0.545, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  94%|█████████▍| 376/400 [03:16<00:11,  2.02it/s, loss=0.0159, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  94%|█████████▍| 377/400 [03:17<00:11,  2.02it/s, loss=0.0159, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  94%|█████████▍| 377/400 [03:17<00:11,  2.02it/s, loss=0.0522, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  94%|█████████▍| 378/400 [03:17<00:10,  2.02it/s, loss=0.0522, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  94%|█████████▍| 378/400 [03:17<00:10,  2.02it/s, loss=0.0254, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  95%|█████████▍| 379/400 [03:18<00:10,  2.02it/s, loss=0.0254, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  95%|█████████▍| 379/400 [03:18<00:10,  2.02it/s, loss=0.00371, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  95%|█████████▌| 380/400 [03:18<00:10,  1.96it/s, loss=0.00371, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  95%|█████████▌| 380/400 [03:18<00:10,  1.96it/s, loss=0.0143, lr=2e-6]\u001b[0m\n",
      "\u001b[34mEnd epoch 19: train_avg_loss=0.06650109562140546\u001b[0m\n",
      "\u001b[34mEpoch: 20\u001b[0m\n",
      "\u001b[34mSteps:  95%|█████████▌| 381/400 [03:19<00:12,  1.56it/s, loss=0.0143, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  95%|█████████▌| 381/400 [03:19<00:12,  1.56it/s, loss=0.0425, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  96%|█████████▌| 382/400 [03:19<00:10,  1.68it/s, loss=0.0425, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  96%|█████████▌| 382/400 [03:20<00:10,  1.68it/s, loss=0.116, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  96%|█████████▌| 383/400 [03:20<00:09,  1.77it/s, loss=0.116, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  96%|█████████▌| 383/400 [03:20<00:09,  1.77it/s, loss=0.133, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  96%|█████████▌| 384/400 [03:20<00:08,  1.84it/s, loss=0.133, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  96%|█████████▌| 384/400 [03:21<00:08,  1.84it/s, loss=0.00627, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  96%|█████████▋| 385/400 [03:21<00:07,  1.89it/s, loss=0.00627, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  96%|█████████▋| 385/400 [03:21<00:07,  1.89it/s, loss=0.0235, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  96%|█████████▋| 386/400 [03:21<00:07,  1.92it/s, loss=0.0235, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  96%|█████████▋| 386/400 [03:22<00:07,  1.92it/s, loss=0.24, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  97%|█████████▋| 387/400 [03:22<00:06,  1.95it/s, loss=0.24, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  97%|█████████▋| 387/400 [03:22<00:06,  1.95it/s, loss=0.156, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  97%|█████████▋| 388/400 [03:22<00:06,  1.98it/s, loss=0.156, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  97%|█████████▋| 388/400 [03:22<00:06,  1.98it/s, loss=0.00695, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  97%|█████████▋| 389/400 [03:23<00:05,  1.99it/s, loss=0.00695, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  97%|█████████▋| 389/400 [03:23<00:05,  1.99it/s, loss=0.179, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  98%|█████████▊| 390/400 [03:23<00:04,  2.00it/s, loss=0.179, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  98%|█████████▊| 390/400 [03:23<00:04,  2.00it/s, loss=0.00831, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  98%|█████████▊| 391/400 [03:24<00:04,  2.00it/s, loss=0.00831, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  98%|█████████▊| 391/400 [03:24<00:04,  2.00it/s, loss=0.06, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  98%|█████████▊| 392/400 [03:24<00:03,  2.01it/s, loss=0.06, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  98%|█████████▊| 392/400 [03:24<00:03,  2.01it/s, loss=0.0221, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  98%|█████████▊| 393/400 [03:25<00:03,  2.01it/s, loss=0.0221, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  98%|█████████▊| 393/400 [03:25<00:03,  2.01it/s, loss=0.0244, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  98%|█████████▊| 394/400 [03:25<00:02,  2.01it/s, loss=0.0244, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  98%|█████████▊| 394/400 [03:25<00:02,  2.01it/s, loss=0.288, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  99%|█████████▉| 395/400 [03:26<00:02,  2.02it/s, loss=0.288, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  99%|█████████▉| 395/400 [03:26<00:02,  2.02it/s, loss=0.111, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  99%|█████████▉| 396/400 [03:26<00:01,  2.02it/s, loss=0.111, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  99%|█████████▉| 396/400 [03:26<00:01,  2.02it/s, loss=0.002, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  99%|█████████▉| 397/400 [03:27<00:01,  2.02it/s, loss=0.002, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps:  99%|█████████▉| 397/400 [03:27<00:01,  2.02it/s, loss=0.0675, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps: 100%|█████████▉| 398/400 [03:27<00:00,  2.03it/s, loss=0.0675, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps: 100%|█████████▉| 398/400 [03:27<00:00,  2.03it/s, loss=0.0148, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps: 100%|█████████▉| 399/400 [03:28<00:00,  1.95it/s, loss=0.0148, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps: 100%|█████████▉| 399/400 [03:28<00:00,  1.95it/s, loss=0.252, lr=2e-6]\u001b[0m\n",
      "\u001b[34mEnd epoch 20: train_avg_loss=0.0922683136978824\u001b[0m\n",
      "\u001b[34mEpoch: 21\u001b[0m\n",
      "\u001b[34mSteps: 100%|██████████| 400/400 [03:29<00:00,  1.56it/s, loss=0.252, lr=2e-6]\u001b[0m\n",
      "\u001b[34mSteps: 100%|██████████| 400/400 [03:29<00:00,  1.56it/s, loss=0.161, lr=2e-6]\u001b[0m\n",
      "\u001b[34mReached max_teps=400. Stopping training now.\u001b[0m\n",
      "\u001b[34mEnd epoch 21: train_avg_loss=0.16075399518013\u001b[0m\n",
      "\u001b[34mSteps: 100%|██████████| 400/400 [03:29<00:00,  1.91it/s, loss=0.161, lr=2e-6]\u001b[0m\n",
      "\u001b[34mConfiguration saved in /opt/ml/model/model_index.json\u001b[0m\n",
      "\u001b[34mConfiguration saved in /opt/ml/model/vae/config.json\u001b[0m\n",
      "\u001b[34mModel weights saved in /opt/ml/model/vae/diffusion_pytorch_model.bin\u001b[0m\n",
      "\u001b[34mConfiguration saved in /opt/ml/model/unet/config.json\u001b[0m\n",
      "\u001b[34mModel weights saved in /opt/ml/model/unet/diffusion_pytorch_model.bin\u001b[0m\n",
      "\u001b[34mConfiguration saved in /opt/ml/model/scheduler/scheduler_config.json\u001b[0m\n",
      "\u001b[34m2023-09-08 17:56:35,750 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-09-08 17:56:35,751 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-09-08 17:56:35,751 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2023-09-08 17:57:13 Uploading - Uploading generated training model\n",
      "2023-09-08 18:02:29 Completed - Training job completed\n",
      "Training seconds: 954\n",
      "Billable seconds: 954\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.utils import name_from_base\n",
    "from sagemaker.tuner import HyperparameterTuner\n",
    "\n",
    "training_job_name = name_from_base(f\"jumpstart-example-{train_model_id}-transfer-learning\")\n",
    "\n",
    "# Create SageMaker Estimator instance\n",
    "sd_estimator = Estimator(\n",
    "    role=aws_role,\n",
    "    image_uri=train_image_uri,\n",
    "    source_dir=train_source_uri,\n",
    "    model_uri=train_model_uri,\n",
    "    entry_point=\"transfer_learning.py\",  # Entry-point file in source_dir and present in train_source_uri.\n",
    "    instance_count=1,\n",
    "    instance_type=training_instance_type,\n",
    "    max_run=360000,\n",
    "    hyperparameters=hyperparameters,\n",
    "    output_path=s3_output_location,\n",
    "    base_job_name=training_job_name,\n",
    ")\n",
    "\n",
    "# Launch a SageMaker Training job by passing s3 path of the training data\n",
    "sd_estimator.fit({\"training\": training_dataset_s3_path}, logs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fadc21e",
   "metadata": {},
   "source": [
    "### 2.4. Deploy and run inference on the fine-tuned model\n",
    "\n",
    "---\n",
    "\n",
    "A trained model does nothing on its own. We now want to use the model to perform inference. For this example, that means predicting the bounding boxes of an image. We follow the same steps as in [2. Run inference on the pre-trained model](#2.-Run-inference-on-the-pre-trained-model). We start by retrieving the jumpstart artifacts for deploying an endpoint. However, instead of base_predictor, we  deploy the `od_estimator` that we fine-tuned.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf25c1e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inference_instance_type = \"ml.g4dn.2xlarge\"\n",
    "\n",
    "# Retrieve the inference docker container uri\n",
    "deploy_image_uri = image_uris.retrieve(\n",
    "    region=None,\n",
    "    framework=None,  # automatically inferred from model_id\n",
    "    image_scope=\"inference\",\n",
    "    model_id=train_model_id,\n",
    "    model_version=train_model_version,\n",
    "    instance_type=inference_instance_type,\n",
    ")\n",
    "# Retrieve the inference script uri. This includes scripts for model loading, inference handling etc.\n",
    "deploy_source_uri = script_uris.retrieve(\n",
    "    model_id=train_model_id, model_version=train_model_version, script_scope=\"inference\"\n",
    ")\n",
    "\n",
    "endpoint_name = name_from_base(f\"jumpstart-example-FT-{train_model_id}-\")\n",
    "\n",
    "# Use the estimator from the previous step to deploy to a SageMaker endpoint\n",
    "finetuned_predictor = sd_estimator.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=inference_instance_type,\n",
    "    entry_point=\"inference.py\",  # entry point file in source_dir and present in deploy_source_uri\n",
    "    image_uri=deploy_image_uri,\n",
    "    source_dir=deploy_source_uri,\n",
    "    endpoint_name=endpoint_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f61ca2-39f8-4449-b5bb-108851b74dba",
   "metadata": {
    "tags": []
   },
   "source": [
    "Next, we query the finetuned model, parse the response and display the generated image. Functions for these are implemented below. Input to the endpoint is any string of text dumped in json and encoded in `utf-8` format. Output of the endpoint is a json with generated text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaae5307-4beb-4f55-a389-a06c89cbc02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def query(model_predictor, text):\n",
    "    \"\"\"Query the model predictor.\"\"\"\n",
    "\n",
    "    encoded_text = text.encode(\"utf-8\")\n",
    "\n",
    "    query_response = model_predictor.predict(\n",
    "        encoded_text,\n",
    "        {\n",
    "            \"ContentType\": \"application/x-text\",\n",
    "            \"Accept\": \"application/json\",\n",
    "        },\n",
    "    )\n",
    "    return query_response\n",
    "\n",
    "\n",
    "def parse_response(query_response):\n",
    "    \"\"\"Parse response and return generated image and the prompt\"\"\"\n",
    "\n",
    "    response_dict = json.loads(query_response)\n",
    "    return response_dict[\"generated_image\"], response_dict[\"prompt\"]\n",
    "\n",
    "\n",
    "def display_img_and_prompt(img, prmpt):\n",
    "    \"\"\"Display hallucinated image.\"\"\"\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    plt.imshow(np.array(img))\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(prmpt)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4bf38f",
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "text = \"a photo of a Doppler dog with a hat\"\n",
    "query_response = query(finetuned_predictor, text)\n",
    "img, prmpt = parse_response(query_response)\n",
    "display_img_and_prompt(img, prmpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944a6f0f",
   "metadata": {},
   "source": [
    "All the parameters mentioned in [2.4. Supported Inference parameters](#2.4.-Supported-Inference-parameters) are supported with finetuned model as well. You may also receive compressed image output as in [2.5. Compressed Image Output](#2.5.-Compressed-Image-Output) by changing `accept`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3381a2c",
   "metadata": {},
   "source": [
    "---\n",
    "Next, we delete the endpoint corresponding to the finetuned model.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03c8594",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Delete the SageMaker endpoint\n",
    "finetuned_predictor.delete_model()\n",
    "finetuned_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a504c9ac",
   "metadata": {},
   "source": [
    "## 3. Conclusion\n",
    "---\n",
    "\n",
    "Although creating impressive images can find use in industries ranging from art to NFTs and beyond, today we also expect AI to be personalizable. JumpStart provides fine-tuning capability to the pre-trained models so that you can adapt the model to your own use case with as little as five training images. This can be useful when creating art, logos, custom designs, NFTs, and so on, or fun stuff such as generating custom AI images of your pets or avatars of yourself. In this lab, we learned how to fine-tune a stable diffusion text to image generation model. To learn more about Stable Diffusion fine-tuning, please check out the blog [Fine-tune text-to-image Stable Diffusion models with Amazon SageMaker JumpStart](https://aws.amazon.com/blogs/machine-learning/fine-tune-text-to-image-stable-diffusion-models-with-amazon-sagemaker-jumpstart/)."
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
